{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e56af9ec",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27f43cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "from nltk import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "\n",
    "import multiprocessing\n",
    "import parmap\n",
    "\n",
    "\n",
    "from datasets import Dataset, load_from_disk, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoConfig, TrainingArguments, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81616053",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data\"\n",
    "model_cache_dir = \"./BERT_cache\"\n",
    "current_time = strftime(\"%Y-%m-%d-%H:%M:%S\", gmtime())\n",
    "model_output_dir = './BERT'+\"_\"+current_time\n",
    "original_train_datasets_path = data_path + \"/original_datasets/TS1\"\n",
    "original_valid_datasets_path = data_path + \"/original_datasets/VS1\"\n",
    "train_sentence_list_file_name = \"train_sentence_list.pickle\"\n",
    "valid_sentence_list_file_name = \"valid_sentence_list.pickle\"\n",
    "raw_datasets_folder_name = \"raw_datasets\"\n",
    "tokenizer_name = \"tokenizer_aihub_news\"\n",
    "tokenized_datasets_folder_name = [\"tokenized_datasets\", \"tokenized_datasets_1\", \"tokenized_datasets_2\", \"tokenized_datasets_3\"]\n",
    "grouped_tokenized_datasets_folder_name = \"grouped_tokenized_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c2e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listize_dataset(json_file_path):    \n",
    "    sentence_list = []\n",
    "\n",
    "    with open(json_file_path, 'r', encoding='UTF-8') as f:\n",
    "        json_object = json.load(f)\n",
    "\n",
    "    for line in json_object['SJML']['text']:\n",
    "        raw_text = line['content'].replace('..', '.')\n",
    "        raw_text_list = sent_tokenize(raw_text)\n",
    "        \n",
    "        for item in raw_text_list:\n",
    "            if len(item) < 5:\n",
    "                continue\n",
    "            else:\n",
    "                sentence_list.append(item.strip())\n",
    "\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6abc2067",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b13aae49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331b16b1b9944553a556c5037e90e579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path_list = []\n",
    "\n",
    "file_list = os.listdir(os.path.join(original_train_datasets_path))\n",
    "for dir_name in file_list:\n",
    "    file_path = os.path.join(original_train_datasets_path, dir_name)\n",
    "\n",
    "    json_file_list = os.listdir(file_path)\n",
    "    for json_file_name in json_file_list:\n",
    "        json_file_path = os.path.join(file_path, json_file_name)\n",
    "\n",
    "        file_path_list.append(json_file_path)\n",
    "\n",
    "train_sentence_list_of_list = parmap.map(listize_dataset, file_path_list, pm_pbar=True, pm_processes=num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b76f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a6ac449d6743cda91d6e86278214f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path_list = []\n",
    "file_list = os.listdir(os.path.join(original_valid_datasets_path))\n",
    "for dir_name in file_list:\n",
    "    file_path = os.path.join(original_valid_datasets_path, dir_name)\n",
    "\n",
    "    json_file_list = os.listdir(file_path)\n",
    "    for json_file_name in json_file_list:\n",
    "        json_file_path = os.path.join(file_path, json_file_name)\n",
    "\n",
    "        file_path_list.append(json_file_path)\n",
    "\n",
    "valid_sentence_list_of_list = parmap.map(listize_dataset, file_path_list, pm_pbar=True, pm_processes=num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb33ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence_list = list(chain(*train_sentence_list_of_list))\n",
    "valid_sentence_list = list(chain(*valid_sentence_list_of_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3ab95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(data_path, train_sentence_list_file_name), 'wb') as fw:\n",
    "#     pickle.dump(train_sentence_list, fw)\n",
    "\n",
    "# with open(os.path.join(data_path, valid_sentence_list_file_name), 'wb') as fw:\n",
    "#     pickle.dump(valid_sentence_list, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa54a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(data_path, train_sentence_list_file_name), 'rb') as f:\n",
    "#     train_sentence_list = pickle.load(f)\n",
    "\n",
    "# with open(os.path.join(data_path, valid_sentence_list_file_name), 'rb') as f:\n",
    "#     valid_sentence_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f9ce7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets = Dataset.from_dict({\"text\":total_sentence_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1e683a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(raw_datasets))\n",
    "# raw_datasets[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d763cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets.save_to_disk(os.path.join(data_path, raw_datasets_folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83b93bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets = load_from_disk(os.path.join(data_path, raw_datasets_folder_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0254e1e2",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc0bae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training a tokenizer from scratch\n",
    "# def batch_iterator(batch_size=10000):\n",
    "#     for i in tqdm(range(0, len(raw_datasets), batch_size)):\n",
    "#         yield raw_datasets[i:i+batch_size][\"text\"]\n",
    "        \n",
    "# old_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = old_tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=15000)\n",
    "# tokenizer.save_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea97ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets_1 = raw_datasets.select(range(20000000))\n",
    "# raw_datasets_2 = raw_datasets.select(range(20000000, 40000000))\n",
    "# raw_datasets_3 = raw_datasets.select(range(40000000, len(raw_datasets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae990d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e82a304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_texts(examples):\n",
    "#     tokenized_inputs = tokenizer(\n",
    "#        examples[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=tokenizer.model_max_length\n",
    "#     )\n",
    "#     return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ab0e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_1 = raw_datasets_1.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "# tokenized_datasets_1.save_to_disk(os.path.join(data_path, tokenized_datasets_folder_name[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bff0963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_2 = raw_datasets_2.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "# tokenized_datasets_2.save_to_disk(os.path.join(data_path, tokenized_datasets_folder_name[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efd6be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_3 = raw_datasets_3.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "# tokenized_datasets_3.save_to_disk(os.path.join(data_path, tokenized_datasets_folder_name[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eed83b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_1 = load_from_disk(os.path.join(data_path, tokenized_datasets_folder_name[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a180cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_2 = load_from_disk(os.path.join(data_path, tokenized_datasets_folder_name[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc8cfe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_3 = load_from_disk(os.path.join(data_path, tokenized_datasets_folder_name[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b13958fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = concatenate_datasets([tokenized_datasets_1, tokenized_datasets_2, tokenized_datasets_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96272b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9068f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets.save_to_disk(os.path.join(data_path, tokenized_datasets_folder_name[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6292082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = load_from_disk(os.path.join(data_path, tokenized_datasets_folder_name[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ec049",
   "metadata": {},
   "source": [
    "# Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1816b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# # max_seq_length.\n",
    "# def group_texts(examples):\n",
    "#     # Concatenate all texts.\n",
    "#     concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "#     total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "#     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "#     # customize this part to your needs.\n",
    "#     if total_length >= tokenizer.model_max_length:\n",
    "#         total_length = (total_length // tokenizer.model_max_length) * tokenizer.model_max_length\n",
    "#     # Split by chunks of max_len.\n",
    "#     result = {\n",
    "#         k: [t[i : i + tokenizer.model_max_length] for i in range(0, total_length, tokenizer.model_max_length)]\n",
    "#         for k, t in concatenated_examples.items()\n",
    "#     }\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81784961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = tokenized_datasets.map(group_texts, batched=True, num_proc=16)\n",
    "# # shuffle dataset\n",
    "# tokenized_datasets = tokenized_datasets.shuffle(seed=34)\n",
    "\n",
    "# print(f\"the dataset contains in total {len(tokenized_datasets)*tokenizer.model_max_length} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd55ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets.save_to_disk(os.path.join(data_path, grouped_tokenized_datasets_folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45020976",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = load_from_disk(os.path.join(data_path, grouped_tokenized_datasets_folder_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0be820bc",
   "metadata": {},
   "source": [
    "# DDP Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aaebc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b16c7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[W reducer.cpp:1278] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1278] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1278] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1278] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.7211, 'learning_rate': 4.994944592736391e-05, 'epoch': 0.0}\n",
      "{'loss': 6.7045, 'learning_rate': 4.989889185472782e-05, 'epoch': 0.0}\n",
      "{'loss': 6.1564, 'learning_rate': 4.984833778209173e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9144, 'learning_rate': 4.979778370945564e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7327, 'learning_rate': 4.974722963681954e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5685, 'learning_rate': 4.969667556418345e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4161, 'learning_rate': 4.964612149154736e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2591, 'learning_rate': 4.959556741891127e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0988, 'learning_rate': 4.9545013346275176e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9543, 'learning_rate': 4.949445927363909e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8139, 'learning_rate': 4.9443905201002995e-05, 'epoch': 0.02}\n",
      "{'loss': 4.697, 'learning_rate': 4.939335112836691e-05, 'epoch': 0.02}\n",
      "{'loss': 4.5746, 'learning_rate': 4.9342797055730814e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4999, 'learning_rate': 4.929224298309472e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4078, 'learning_rate': 4.9241688910458626e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3381, 'learning_rate': 4.919113483782254e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2723, 'learning_rate': 4.9140580765186444e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2121, 'learning_rate': 4.909002669255036e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1442, 'learning_rate': 4.903947261991426e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1, 'learning_rate': 4.8988918547278176e-05, 'epoch': 0.04}\n",
      "{'loss': 4.036, 'learning_rate': 4.893836447464208e-05, 'epoch': 0.04}\n",
      "{'loss': 3.9968, 'learning_rate': 4.888781040200599e-05, 'epoch': 0.04}\n",
      "{'loss': 3.9432, 'learning_rate': 4.8837256329369894e-05, 'epoch': 0.05}\n",
      "{'loss': 3.9017, 'learning_rate': 4.8786702256733806e-05, 'epoch': 0.05}\n",
      "{'loss': 3.8682, 'learning_rate': 4.873614818409771e-05, 'epoch': 0.05}\n",
      "{'loss': 3.8084, 'learning_rate': 4.8685594111461625e-05, 'epoch': 0.05}\n",
      "{'loss': 3.7711, 'learning_rate': 4.863504003882553e-05, 'epoch': 0.05}\n",
      "{'loss': 3.7282, 'learning_rate': 4.858448596618944e-05, 'epoch': 0.06}\n",
      "{'loss': 3.6904, 'learning_rate': 4.853393189355335e-05, 'epoch': 0.06}\n",
      "{'loss': 3.6605, 'learning_rate': 4.8483377820917256e-05, 'epoch': 0.06}\n",
      "{'loss': 3.6252, 'learning_rate': 4.843282374828116e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5956, 'learning_rate': 4.838226967564507e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5589, 'learning_rate': 4.833171560300898e-05, 'epoch': 0.07}\n",
      "{'loss': 3.53, 'learning_rate': 4.8281161530372886e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5016, 'learning_rate': 4.82306074577368e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4786, 'learning_rate': 4.8180053385100705e-05, 'epoch': 0.07}\n",
      "{'loss': 3.443, 'learning_rate': 4.812949931246462e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4243, 'learning_rate': 4.8078945239828524e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4002, 'learning_rate': 4.802839116719243e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3618, 'learning_rate': 4.7977837094556336e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3515, 'learning_rate': 4.792728302192025e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3257, 'learning_rate': 4.7876728949284154e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3005, 'learning_rate': 4.782617487664807e-05, 'epoch': 0.09}\n",
      "{'loss': 3.2759, 'learning_rate': 4.777562080401197e-05, 'epoch': 0.09}\n",
      "{'loss': 3.2533, 'learning_rate': 4.7725066731375886e-05, 'epoch': 0.09}\n",
      "{'loss': 3.2311, 'learning_rate': 4.767451265873979e-05, 'epoch': 0.09}\n",
      "{'loss': 3.2132, 'learning_rate': 4.76239585861037e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2023, 'learning_rate': 4.7573404513467604e-05, 'epoch': 0.1}\n",
      "{'loss': 3.1804, 'learning_rate': 4.7522850440831516e-05, 'epoch': 0.1}\n",
      "{'loss': 3.1622, 'learning_rate': 4.747229636819542e-05, 'epoch': 0.1}\n",
      "{'loss': 3.1484, 'learning_rate': 4.7421742295559335e-05, 'epoch': 0.1}\n",
      "{'loss': 3.1256, 'learning_rate': 4.737118822292324e-05, 'epoch': 0.11}\n",
      "{'loss': 3.1158, 'learning_rate': 4.7320634150287154e-05, 'epoch': 0.11}\n",
      "{'loss': 3.0995, 'learning_rate': 4.727008007765106e-05, 'epoch': 0.11}\n",
      "{'loss': 3.0658, 'learning_rate': 4.721952600501497e-05, 'epoch': 0.11}\n",
      "{'loss': 3.0495, 'learning_rate': 4.716897193237887e-05, 'epoch': 0.11}\n",
      "{'loss': 3.0485, 'learning_rate': 4.711841785974278e-05, 'epoch': 0.12}\n",
      "{'loss': 3.0383, 'learning_rate': 4.706786378710669e-05, 'epoch': 0.12}\n",
      "{'loss': 3.0087, 'learning_rate': 4.7017309714470596e-05, 'epoch': 0.12}\n",
      "{'loss': 3.0078, 'learning_rate': 4.696675564183451e-05, 'epoch': 0.12}\n",
      "{'loss': 2.9895, 'learning_rate': 4.6916201569198415e-05, 'epoch': 0.12}\n",
      "{'loss': 2.9844, 'learning_rate': 4.686564749656233e-05, 'epoch': 0.13}\n",
      "{'loss': 2.9761, 'learning_rate': 4.6815093423926234e-05, 'epoch': 0.13}\n",
      "{'loss': 2.9538, 'learning_rate': 4.6764539351290146e-05, 'epoch': 0.13}\n",
      "{'loss': 2.9299, 'learning_rate': 4.6713985278654045e-05, 'epoch': 0.13}\n",
      "{'loss': 2.9268, 'learning_rate': 4.666343120601796e-05, 'epoch': 0.13}\n",
      "{'loss': 2.9228, 'learning_rate': 4.6612877133381864e-05, 'epoch': 0.14}\n",
      "{'loss': 2.8986, 'learning_rate': 4.656232306074578e-05, 'epoch': 0.14}\n",
      "{'loss': 2.8947, 'learning_rate': 4.651176898810968e-05, 'epoch': 0.14}\n",
      "{'loss': 2.8707, 'learning_rate': 4.6461214915473596e-05, 'epoch': 0.14}\n",
      "{'loss': 2.857, 'learning_rate': 4.64106608428375e-05, 'epoch': 0.14}\n",
      "{'loss': 2.8657, 'learning_rate': 4.6360106770201414e-05, 'epoch': 0.15}\n",
      "{'loss': 2.8436, 'learning_rate': 4.630955269756532e-05, 'epoch': 0.15}\n",
      "{'loss': 2.83, 'learning_rate': 4.6258998624929226e-05, 'epoch': 0.15}\n",
      "{'loss': 2.8297, 'learning_rate': 4.620844455229313e-05, 'epoch': 0.15}\n",
      "{'loss': 2.8064, 'learning_rate': 4.6157890479657045e-05, 'epoch': 0.15}\n",
      "{'loss': 2.8046, 'learning_rate': 4.610733640702095e-05, 'epoch': 0.16}\n",
      "{'loss': 2.7962, 'learning_rate': 4.6056782334384864e-05, 'epoch': 0.16}\n",
      "{'loss': 2.7831, 'learning_rate': 4.600622826174877e-05, 'epoch': 0.16}\n",
      "{'loss': 2.7692, 'learning_rate': 4.595567418911268e-05, 'epoch': 0.16}\n",
      "{'loss': 2.7682, 'learning_rate': 4.590512011647659e-05, 'epoch': 0.16}\n",
      "{'loss': 2.7516, 'learning_rate': 4.5854566043840494e-05, 'epoch': 0.17}\n",
      "{'loss': 2.7421, 'learning_rate': 4.58040119712044e-05, 'epoch': 0.17}\n",
      "{'loss': 2.7352, 'learning_rate': 4.5753457898568306e-05, 'epoch': 0.17}\n",
      "{'loss': 2.7285, 'learning_rate': 4.570290382593222e-05, 'epoch': 0.17}\n",
      "{'loss': 2.7131, 'learning_rate': 4.5652349753296125e-05, 'epoch': 0.17}\n",
      "{'loss': 2.7118, 'learning_rate': 4.560179568066004e-05, 'epoch': 0.18}\n",
      "{'loss': 2.7023, 'learning_rate': 4.5551241608023943e-05, 'epoch': 0.18}\n",
      "{'loss': 2.6998, 'learning_rate': 4.5500687535387856e-05, 'epoch': 0.18}\n",
      "{'loss': 2.6886, 'learning_rate': 4.545013346275176e-05, 'epoch': 0.18}\n",
      "{'loss': 2.6845, 'learning_rate': 4.539957939011567e-05, 'epoch': 0.18}\n",
      "{'loss': 2.6713, 'learning_rate': 4.5349025317479574e-05, 'epoch': 0.19}\n",
      "{'loss': 2.6682, 'learning_rate': 4.529847124484349e-05, 'epoch': 0.19}\n",
      "{'loss': 2.6575, 'learning_rate': 4.524791717220739e-05, 'epoch': 0.19}\n",
      "{'loss': 2.6536, 'learning_rate': 4.5197363099571305e-05, 'epoch': 0.19}\n",
      "{'loss': 2.6463, 'learning_rate': 4.514680902693521e-05, 'epoch': 0.19}\n",
      "{'loss': 2.6363, 'learning_rate': 4.5096254954299124e-05, 'epoch': 0.2}\n",
      "{'loss': 2.6347, 'learning_rate': 4.504570088166303e-05, 'epoch': 0.2}\n",
      "{'loss': 2.6238, 'learning_rate': 4.4995146809026936e-05, 'epoch': 0.2}\n",
      "{'loss': 2.619, 'learning_rate': 4.494459273639084e-05, 'epoch': 0.2}\n",
      "{'loss': 2.6129, 'learning_rate': 4.4894038663754755e-05, 'epoch': 0.2}\n",
      "{'loss': 2.6053, 'learning_rate': 4.484348459111866e-05, 'epoch': 0.21}\n",
      "{'loss': 2.5992, 'learning_rate': 4.4792930518482573e-05, 'epoch': 0.21}\n",
      "{'loss': 2.5881, 'learning_rate': 4.474237644584648e-05, 'epoch': 0.21}\n",
      "{'loss': 2.583, 'learning_rate': 4.469182237321039e-05, 'epoch': 0.21}\n",
      "{'loss': 2.5785, 'learning_rate': 4.46412683005743e-05, 'epoch': 0.21}\n",
      "{'loss': 2.5676, 'learning_rate': 4.4590714227938204e-05, 'epoch': 0.22}\n",
      "{'loss': 2.5755, 'learning_rate': 4.454016015530211e-05, 'epoch': 0.22}\n",
      "{'loss': 2.5649, 'learning_rate': 4.448960608266602e-05, 'epoch': 0.22}\n",
      "{'loss': 2.5421, 'learning_rate': 4.443905201002993e-05, 'epoch': 0.22}\n",
      "{'loss': 2.5562, 'learning_rate': 4.4388497937393835e-05, 'epoch': 0.22}\n",
      "{'loss': 2.5465, 'learning_rate': 4.433794386475775e-05, 'epoch': 0.23}\n",
      "{'loss': 2.5419, 'learning_rate': 4.428738979212165e-05, 'epoch': 0.23}\n",
      "{'loss': 2.5341, 'learning_rate': 4.4236835719485566e-05, 'epoch': 0.23}\n",
      "{'loss': 2.5174, 'learning_rate': 4.418628164684947e-05, 'epoch': 0.23}\n",
      "{'loss': 2.5299, 'learning_rate': 4.413572757421338e-05, 'epoch': 0.23}\n",
      "{'loss': 2.5103, 'learning_rate': 4.4085173501577284e-05, 'epoch': 0.24}\n",
      "{'loss': 2.5147, 'learning_rate': 4.40346194289412e-05, 'epoch': 0.24}\n",
      "{'loss': 2.4969, 'learning_rate': 4.39840653563051e-05, 'epoch': 0.24}\n",
      "{'loss': 2.497, 'learning_rate': 4.3933511283669015e-05, 'epoch': 0.24}\n",
      "{'loss': 2.4943, 'learning_rate': 4.388295721103292e-05, 'epoch': 0.24}\n",
      "{'loss': 2.49, 'learning_rate': 4.3832403138396834e-05, 'epoch': 0.25}\n",
      "{'loss': 2.4822, 'learning_rate': 4.378184906576074e-05, 'epoch': 0.25}\n",
      "{'loss': 2.4775, 'learning_rate': 4.373129499312465e-05, 'epoch': 0.25}\n",
      "{'loss': 2.4725, 'learning_rate': 4.368074092048856e-05, 'epoch': 0.25}\n",
      "{'loss': 2.4759, 'learning_rate': 4.3630186847852465e-05, 'epoch': 0.25}\n",
      "{'loss': 2.4589, 'learning_rate': 4.357963277521637e-05, 'epoch': 0.26}\n",
      "{'loss': 2.4573, 'learning_rate': 4.352907870258028e-05, 'epoch': 0.26}\n",
      "{'loss': 2.4511, 'learning_rate': 4.347852462994419e-05, 'epoch': 0.26}\n",
      "{'loss': 2.4408, 'learning_rate': 4.34279705573081e-05, 'epoch': 0.26}\n",
      "{'loss': 2.4443, 'learning_rate': 4.337741648467201e-05, 'epoch': 0.26}\n",
      "{'loss': 2.4429, 'learning_rate': 4.332686241203592e-05, 'epoch': 0.27}\n",
      "{'loss': 2.4306, 'learning_rate': 4.327630833939983e-05, 'epoch': 0.27}\n",
      "{'loss': 2.4161, 'learning_rate': 4.322575426676373e-05, 'epoch': 0.27}\n",
      "{'loss': 2.4262, 'learning_rate': 4.317520019412764e-05, 'epoch': 0.27}\n",
      "{'loss': 2.4216, 'learning_rate': 4.312464612149155e-05, 'epoch': 0.28}\n",
      "{'loss': 2.4093, 'learning_rate': 4.307409204885546e-05, 'epoch': 0.28}\n",
      "{'loss': 2.4099, 'learning_rate': 4.302353797621936e-05, 'epoch': 0.28}\n",
      "{'loss': 2.4181, 'learning_rate': 4.2972983903583276e-05, 'epoch': 0.28}\n",
      "{'loss': 2.4077, 'learning_rate': 4.292242983094718e-05, 'epoch': 0.28}\n",
      "{'loss': 2.3906, 'learning_rate': 4.2871875758311095e-05, 'epoch': 0.29}\n",
      "{'loss': 2.3988, 'learning_rate': 4.2821321685675e-05, 'epoch': 0.29}\n",
      "{'loss': 2.3972, 'learning_rate': 4.2770767613038907e-05, 'epoch': 0.29}\n",
      "{'loss': 2.3902, 'learning_rate': 4.272021354040281e-05, 'epoch': 0.29}\n",
      "{'loss': 2.3813, 'learning_rate': 4.2669659467766725e-05, 'epoch': 0.29}\n",
      "{'loss': 2.3787, 'learning_rate': 4.261910539513063e-05, 'epoch': 0.3}\n",
      "{'loss': 2.3661, 'learning_rate': 4.2568551322494544e-05, 'epoch': 0.3}\n",
      "{'loss': 2.375, 'learning_rate': 4.251799724985845e-05, 'epoch': 0.3}\n",
      "{'loss': 2.3705, 'learning_rate': 4.246744317722236e-05, 'epoch': 0.3}\n",
      "{'loss': 2.3649, 'learning_rate': 4.241688910458627e-05, 'epoch': 0.3}\n",
      "{'loss': 2.3634, 'learning_rate': 4.2366335031950175e-05, 'epoch': 0.31}\n",
      "{'loss': 2.3579, 'learning_rate': 4.231578095931408e-05, 'epoch': 0.31}\n",
      "{'loss': 2.3446, 'learning_rate': 4.226522688667799e-05, 'epoch': 0.31}\n",
      "{'loss': 2.3555, 'learning_rate': 4.22146728140419e-05, 'epoch': 0.31}\n",
      "{'loss': 2.3442, 'learning_rate': 4.216411874140581e-05, 'epoch': 0.31}\n",
      "{'loss': 2.3421, 'learning_rate': 4.211356466876972e-05, 'epoch': 0.32}\n",
      "{'loss': 2.3353, 'learning_rate': 4.206301059613363e-05, 'epoch': 0.32}\n",
      "{'loss': 2.3301, 'learning_rate': 4.2012456523497537e-05, 'epoch': 0.32}\n",
      "{'loss': 2.3454, 'learning_rate': 4.196190245086144e-05, 'epoch': 0.32}\n",
      "{'loss': 2.3409, 'learning_rate': 4.191134837822535e-05, 'epoch': 0.32}\n",
      "{'loss': 2.3274, 'learning_rate': 4.186079430558926e-05, 'epoch': 0.33}\n",
      "{'loss': 2.3208, 'learning_rate': 4.181024023295317e-05, 'epoch': 0.33}\n",
      "{'loss': 2.3246, 'learning_rate': 4.175968616031708e-05, 'epoch': 0.33}\n",
      "{'loss': 2.3024, 'learning_rate': 4.1709132087680986e-05, 'epoch': 0.33}\n",
      "{'loss': 2.3142, 'learning_rate': 4.16585780150449e-05, 'epoch': 0.33}\n",
      "{'loss': 2.3157, 'learning_rate': 4.1608023942408805e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3087, 'learning_rate': 4.155746986977271e-05, 'epoch': 0.34}\n",
      "{'loss': 2.2997, 'learning_rate': 4.1506915797136617e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3016, 'learning_rate': 4.145636172450052e-05, 'epoch': 0.34}\n",
      "{'loss': 2.2958, 'learning_rate': 4.1405807651864435e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3002, 'learning_rate': 4.135525357922834e-05, 'epoch': 0.35}\n",
      "{'loss': 2.2802, 'learning_rate': 4.1304699506592254e-05, 'epoch': 0.35}\n",
      "{'loss': 2.2755, 'learning_rate': 4.125414543395616e-05, 'epoch': 0.35}\n",
      "{'loss': 2.2768, 'learning_rate': 4.120359136132007e-05, 'epoch': 0.35}\n",
      "{'loss': 2.2757, 'learning_rate': 4.115303728868398e-05, 'epoch': 0.35}\n",
      "{'loss': 2.2754, 'learning_rate': 4.110248321604789e-05, 'epoch': 0.36}\n",
      "{'loss': 2.2692, 'learning_rate': 4.105192914341179e-05, 'epoch': 0.36}\n",
      "{'loss': 2.2763, 'learning_rate': 4.10013750707757e-05, 'epoch': 0.36}\n",
      "{'loss': 2.2673, 'learning_rate': 4.095082099813961e-05, 'epoch': 0.36}\n",
      "{'loss': 2.2621, 'learning_rate': 4.090026692550352e-05, 'epoch': 0.36}\n",
      "{'loss': 2.2648, 'learning_rate': 4.084971285286743e-05, 'epoch': 0.37}\n",
      "{'loss': 2.2458, 'learning_rate': 4.079915878023134e-05, 'epoch': 0.37}\n",
      "{'loss': 2.2468, 'learning_rate': 4.0748604707595247e-05, 'epoch': 0.37}\n",
      "{'loss': 2.2526, 'learning_rate': 4.069805063495916e-05, 'epoch': 0.37}\n",
      "{'loss': 2.2539, 'learning_rate': 4.0647496562323065e-05, 'epoch': 0.37}\n",
      "{'loss': 2.2442, 'learning_rate': 4.059694248968697e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2434, 'learning_rate': 4.054638841705088e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 39\u001b[0m\n\u001b[1;32m     30\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     31\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     32\u001b[0m         args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m     33\u001b[0m         data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[1;32m     34\u001b[0m         train_dataset\u001b[39m=\u001b[39mtokenized_datasets,\n\u001b[1;32m     35\u001b[0m     )   \n\u001b[1;32m     37\u001b[0m     trainer\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 39\u001b[0m notebook_launcher(train_trainer_ddp, args\u001b[39m=\u001b[39;49m(), num_processes\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/launchers.py:136\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port)\u001b[0m\n\u001b[1;32m    133\u001b[0m         launcher \u001b[39m=\u001b[39m PrepareForLaunch(function, distributed_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMULTI_GPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLaunching training on \u001b[39m\u001b[39m{\u001b[39;00mnum_processes\u001b[39m}\u001b[39;00m\u001b[39m GPUs.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m         start_processes(launcher, args\u001b[39m=\u001b[39;49margs, nprocs\u001b[39m=\u001b[39;49mnum_processes, start_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfork\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    138\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[39m# No need for a distributed launch otherwise as it's either CPU, GPU or MPS.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[39mif\u001b[39;00m is_mps_available():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:198\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    197\u001b[0m \u001b[39m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    199\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:109\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m# Wait for any process to fail or all of them to succeed.\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m ready \u001b[39m=\u001b[39m multiprocessing\u001b[39m.\u001b[39;49mconnection\u001b[39m.\u001b[39;49mwait(\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentinels\u001b[39m.\u001b[39;49mkeys(),\n\u001b[1;32m    111\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m error_index \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39mfor\u001b[39;00m sentinel \u001b[39min\u001b[39;00m ready:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2576, 'learning_rate': 4.049583434441479e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2298, 'learning_rate': 4.0445280271778696e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2318, 'learning_rate': 4.039472619914261e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2288, 'learning_rate': 4.0344172126506514e-05, 'epoch': 0.39}\n",
      "{'loss': 2.2412, 'learning_rate': 4.029361805387043e-05, 'epoch': 0.39}\n",
      "{'loss': 2.2214, 'learning_rate': 4.024306398123433e-05, 'epoch': 0.39}\n",
      "{'loss': 2.2224, 'learning_rate': 4.019250990859824e-05, 'epoch': 0.39}\n",
      "{'loss': 2.2284, 'learning_rate': 4.0141955835962145e-05, 'epoch': 0.39}\n",
      "{'loss': 2.2156, 'learning_rate': 4.009140176332605e-05, 'epoch': 0.4}\n",
      "{'loss': 2.2186, 'learning_rate': 4.0040847690689964e-05, 'epoch': 0.4}\n",
      "{'loss': 2.2218, 'learning_rate': 3.999029361805387e-05, 'epoch': 0.4}\n",
      "{'loss': 2.2168, 'learning_rate': 3.993973954541778e-05, 'epoch': 0.4}\n",
      "{'loss': 2.2098, 'learning_rate': 3.988918547278169e-05, 'epoch': 0.4}\n",
      "{'loss': 2.2, 'learning_rate': 3.98386314001456e-05, 'epoch': 0.41}\n",
      "{'loss': 2.2041, 'learning_rate': 3.978807732750951e-05, 'epoch': 0.41}\n",
      "{'loss': 2.2002, 'learning_rate': 3.973752325487341e-05, 'epoch': 0.41}\n",
      "{'loss': 2.1933, 'learning_rate': 3.968696918223732e-05, 'epoch': 0.41}\n",
      "{'loss': 2.1908, 'learning_rate': 3.963641510960123e-05, 'epoch': 0.41}\n",
      "{'loss': 2.197, 'learning_rate': 3.958586103696514e-05, 'epoch': 0.42}\n",
      "{'loss': 2.1989, 'learning_rate': 3.953530696432905e-05, 'epoch': 0.42}\n",
      "{'loss': 2.1882, 'learning_rate': 3.9484752891692956e-05, 'epoch': 0.42}\n",
      "{'loss': 2.1807, 'learning_rate': 3.943419881905687e-05, 'epoch': 0.42}\n",
      "{'loss': 2.1785, 'learning_rate': 3.9383644746420775e-05, 'epoch': 0.42}\n",
      "{'loss': 2.1717, 'learning_rate': 3.933309067378468e-05, 'epoch': 0.43}\n",
      "{'loss': 2.1826, 'learning_rate': 3.928253660114859e-05, 'epoch': 0.43}\n",
      "{'loss': 2.1806, 'learning_rate': 3.92319825285125e-05, 'epoch': 0.43}\n",
      "{'loss': 2.1766, 'learning_rate': 3.9181428455876406e-05, 'epoch': 0.43}\n",
      "{'loss': 2.1713, 'learning_rate': 3.913087438324032e-05, 'epoch': 0.43}\n",
      "{'loss': 2.1687, 'learning_rate': 3.9080320310604224e-05, 'epoch': 0.44}\n",
      "{'loss': 2.1665, 'learning_rate': 3.902976623796814e-05, 'epoch': 0.44}\n",
      "{'loss': 2.1725, 'learning_rate': 3.897921216533204e-05, 'epoch': 0.44}\n",
      "{'loss': 2.1672, 'learning_rate': 3.892865809269595e-05, 'epoch': 0.44}\n",
      "{'loss': 2.1592, 'learning_rate': 3.8878104020059855e-05, 'epoch': 0.44}\n",
      "{'loss': 2.1669, 'learning_rate': 3.882754994742376e-05, 'epoch': 0.45}\n",
      "{'loss': 2.1503, 'learning_rate': 3.8776995874787674e-05, 'epoch': 0.45}\n",
      "{'loss': 2.1584, 'learning_rate': 3.872644180215158e-05, 'epoch': 0.45}\n",
      "{'loss': 2.1452, 'learning_rate': 3.867588772951549e-05, 'epoch': 0.45}\n",
      "{'loss': 2.1487, 'learning_rate': 3.86253336568794e-05, 'epoch': 0.45}\n",
      "{'loss': 2.1387, 'learning_rate': 3.857477958424331e-05, 'epoch': 0.46}\n",
      "{'loss': 2.1435, 'learning_rate': 3.852422551160722e-05, 'epoch': 0.46}\n",
      "{'loss': 2.1374, 'learning_rate': 3.847367143897112e-05, 'epoch': 0.46}\n",
      "{'loss': 2.1419, 'learning_rate': 3.842311736633503e-05, 'epoch': 0.46}\n",
      "{'loss': 2.1455, 'learning_rate': 3.837256329369894e-05, 'epoch': 0.47}\n",
      "{'loss': 2.1376, 'learning_rate': 3.832200922106285e-05, 'epoch': 0.47}\n",
      "{'loss': 2.1307, 'learning_rate': 3.827145514842676e-05, 'epoch': 0.47}\n",
      "{'loss': 2.1269, 'learning_rate': 3.8220901075790666e-05, 'epoch': 0.47}\n",
      "{'loss': 2.1315, 'learning_rate': 3.817034700315458e-05, 'epoch': 0.47}\n",
      "{'loss': 2.126, 'learning_rate': 3.8119792930518485e-05, 'epoch': 0.48}\n",
      "{'loss': 2.1266, 'learning_rate': 3.80692388578824e-05, 'epoch': 0.48}\n",
      "{'loss': 2.1324, 'learning_rate': 3.80186847852463e-05, 'epoch': 0.48}\n",
      "{'loss': 2.1203, 'learning_rate': 3.796813071261021e-05, 'epoch': 0.48}\n",
      "{'loss': 2.1141, 'learning_rate': 3.7917576639974116e-05, 'epoch': 0.48}\n",
      "{'loss': 2.1158, 'learning_rate': 3.786702256733803e-05, 'epoch': 0.49}\n",
      "{'loss': 2.1164, 'learning_rate': 3.7816468494701934e-05, 'epoch': 0.49}\n",
      "{'loss': 2.1142, 'learning_rate': 3.776591442206585e-05, 'epoch': 0.49}\n",
      "{'loss': 2.1058, 'learning_rate': 3.771536034942975e-05, 'epoch': 0.49}\n",
      "{'loss': 2.1184, 'learning_rate': 3.7664806276793666e-05, 'epoch': 0.49}\n",
      "{'loss': 2.1131, 'learning_rate': 3.761425220415757e-05, 'epoch': 0.5}\n",
      "{'loss': 2.1046, 'learning_rate': 3.756369813152148e-05, 'epoch': 0.5}\n",
      "{'loss': 2.1037, 'learning_rate': 3.7513144058885384e-05, 'epoch': 0.5}\n",
      "{'loss': 2.106, 'learning_rate': 3.746258998624929e-05, 'epoch': 0.5}\n",
      "{'loss': 2.106, 'learning_rate': 3.74120359136132e-05, 'epoch': 0.5}\n",
      "{'loss': 2.1047, 'learning_rate': 3.736148184097711e-05, 'epoch': 0.51}\n",
      "{'loss': 2.107, 'learning_rate': 3.731092776834102e-05, 'epoch': 0.51}\n",
      "{'loss': 2.0956, 'learning_rate': 3.726037369570493e-05, 'epoch': 0.51}\n",
      "{'loss': 2.0939, 'learning_rate': 3.720981962306884e-05, 'epoch': 0.51}\n",
      "{'loss': 2.0942, 'learning_rate': 3.7159265550432746e-05, 'epoch': 0.51}\n",
      "{'loss': 2.091, 'learning_rate': 3.710871147779665e-05, 'epoch': 0.52}\n",
      "{'loss': 2.0847, 'learning_rate': 3.705815740516056e-05, 'epoch': 0.52}\n",
      "{'loss': 2.0797, 'learning_rate': 3.700760333252447e-05, 'epoch': 0.52}\n",
      "{'loss': 2.092, 'learning_rate': 3.6957049259888376e-05, 'epoch': 0.52}\n",
      "{'loss': 2.0859, 'learning_rate': 3.690649518725229e-05, 'epoch': 0.52}\n",
      "{'loss': 2.0796, 'learning_rate': 3.6855941114616195e-05, 'epoch': 0.53}\n",
      "{'loss': 2.0733, 'learning_rate': 3.680538704198011e-05, 'epoch': 0.53}\n",
      "{'loss': 2.0729, 'learning_rate': 3.6754832969344014e-05, 'epoch': 0.53}\n",
      "{'loss': 2.0778, 'learning_rate': 3.670427889670792e-05, 'epoch': 0.53}\n",
      "{'loss': 2.0803, 'learning_rate': 3.6653724824071826e-05, 'epoch': 0.53}\n",
      "{'loss': 2.0755, 'learning_rate': 3.660317075143574e-05, 'epoch': 0.54}\n",
      "{'loss': 2.0795, 'learning_rate': 3.6552616678799644e-05, 'epoch': 0.54}\n",
      "{'loss': 2.0649, 'learning_rate': 3.650206260616356e-05, 'epoch': 0.54}\n",
      "{'loss': 2.0671, 'learning_rate': 3.645150853352746e-05, 'epoch': 0.54}\n",
      "{'loss': 2.0738, 'learning_rate': 3.6400954460891376e-05, 'epoch': 0.54}\n",
      "{'loss': 2.0653, 'learning_rate': 3.635040038825528e-05, 'epoch': 0.55}\n",
      "{'loss': 2.0642, 'learning_rate': 3.629984631561919e-05, 'epoch': 0.55}\n",
      "{'loss': 2.0688, 'learning_rate': 3.6249292242983094e-05, 'epoch': 0.55}\n",
      "{'loss': 2.0525, 'learning_rate': 3.6198738170347006e-05, 'epoch': 0.55}\n",
      "{'loss': 2.0565, 'learning_rate': 3.614818409771091e-05, 'epoch': 0.55}\n",
      "{'loss': 2.0575, 'learning_rate': 3.6097630025074825e-05, 'epoch': 0.56}\n",
      "{'loss': 2.0625, 'learning_rate': 3.604707595243873e-05, 'epoch': 0.56}\n",
      "{'loss': 2.0563, 'learning_rate': 3.599652187980264e-05, 'epoch': 0.56}\n",
      "{'loss': 2.043, 'learning_rate': 3.594596780716655e-05, 'epoch': 0.56}\n",
      "{'loss': 2.0458, 'learning_rate': 3.5895413734530456e-05, 'epoch': 0.56}\n",
      "{'loss': 2.0479, 'learning_rate': 3.584485966189436e-05, 'epoch': 0.57}\n",
      "{'loss': 2.0491, 'learning_rate': 3.579430558925827e-05, 'epoch': 0.57}\n",
      "{'loss': 2.0421, 'learning_rate': 3.574375151662218e-05, 'epoch': 0.57}\n",
      "{'loss': 2.0418, 'learning_rate': 3.5693197443986086e-05, 'epoch': 0.57}\n",
      "{'loss': 2.0509, 'learning_rate': 3.564264337135e-05, 'epoch': 0.57}\n",
      "{'loss': 2.0413, 'learning_rate': 3.5592089298713905e-05, 'epoch': 0.58}\n",
      "{'loss': 2.0367, 'learning_rate': 3.554153522607782e-05, 'epoch': 0.58}\n",
      "{'loss': 2.0402, 'learning_rate': 3.5490981153441724e-05, 'epoch': 0.58}\n",
      "{'loss': 2.0365, 'learning_rate': 3.544042708080563e-05, 'epoch': 0.58}\n",
      "{'loss': 2.0287, 'learning_rate': 3.5389873008169535e-05, 'epoch': 0.58}\n",
      "{'loss': 2.0353, 'learning_rate': 3.533931893553345e-05, 'epoch': 0.59}\n",
      "{'loss': 2.0273, 'learning_rate': 3.5288764862897354e-05, 'epoch': 0.59}\n",
      "{'loss': 2.0278, 'learning_rate': 3.523821079026127e-05, 'epoch': 0.59}\n",
      "{'loss': 2.0388, 'learning_rate': 3.518765671762517e-05, 'epoch': 0.59}\n",
      "{'loss': 2.0353, 'learning_rate': 3.5137102644989086e-05, 'epoch': 0.59}\n",
      "{'loss': 2.0318, 'learning_rate': 3.508654857235299e-05, 'epoch': 0.6}\n",
      "{'loss': 2.0147, 'learning_rate': 3.5035994499716904e-05, 'epoch': 0.6}\n",
      "{'loss': 2.0164, 'learning_rate': 3.4985440427080803e-05, 'epoch': 0.6}\n",
      "{'loss': 2.0202, 'learning_rate': 3.4934886354444716e-05, 'epoch': 0.6}\n",
      "{'loss': 2.0345, 'learning_rate': 3.488433228180862e-05, 'epoch': 0.6}\n",
      "{'loss': 2.0205, 'learning_rate': 3.4833778209172535e-05, 'epoch': 0.61}\n",
      "{'loss': 2.0127, 'learning_rate': 3.478322413653644e-05, 'epoch': 0.61}\n",
      "{'loss': 2.0193, 'learning_rate': 3.4732670063900354e-05, 'epoch': 0.61}\n",
      "{'loss': 2.0133, 'learning_rate': 3.468211599126426e-05, 'epoch': 0.61}\n",
      "{'loss': 2.0147, 'learning_rate': 3.4631561918628165e-05, 'epoch': 0.61}\n",
      "{'loss': 2.0085, 'learning_rate': 3.458100784599208e-05, 'epoch': 0.62}\n",
      "{'loss': 2.0134, 'learning_rate': 3.4530453773355984e-05, 'epoch': 0.62}\n",
      "{'loss': 2.0094, 'learning_rate': 3.447989970071989e-05, 'epoch': 0.62}\n",
      "{'loss': 2.0061, 'learning_rate': 3.4429345628083796e-05, 'epoch': 0.62}\n",
      "{'loss': 2.0025, 'learning_rate': 3.437879155544771e-05, 'epoch': 0.62}\n",
      "{'loss': 2.0112, 'learning_rate': 3.4328237482811615e-05, 'epoch': 0.63}\n",
      "{'loss': 2.0065, 'learning_rate': 3.427768341017553e-05, 'epoch': 0.63}\n",
      "{'loss': 2.0108, 'learning_rate': 3.4227129337539433e-05, 'epoch': 0.63}\n",
      "{'loss': 1.9997, 'learning_rate': 3.4176575264903346e-05, 'epoch': 0.63}\n",
      "{'loss': 1.9961, 'learning_rate': 3.412602119226725e-05, 'epoch': 0.63}\n",
      "{'loss': 1.9948, 'learning_rate': 3.407546711963116e-05, 'epoch': 0.64}\n",
      "{'loss': 2.0057, 'learning_rate': 3.4024913046995064e-05, 'epoch': 0.64}\n",
      "{'loss': 1.9925, 'learning_rate': 3.397435897435898e-05, 'epoch': 0.64}\n",
      "{'loss': 2.0037, 'learning_rate': 3.392380490172288e-05, 'epoch': 0.64}\n",
      "{'loss': 1.9951, 'learning_rate': 3.3873250829086795e-05, 'epoch': 0.65}\n",
      "{'loss': 1.9901, 'learning_rate': 3.38226967564507e-05, 'epoch': 0.65}\n",
      "{'loss': 1.9853, 'learning_rate': 3.3772142683814614e-05, 'epoch': 0.65}\n",
      "{'loss': 1.9919, 'learning_rate': 3.372158861117852e-05, 'epoch': 0.65}\n",
      "{'loss': 1.9908, 'learning_rate': 3.3671034538542426e-05, 'epoch': 0.65}\n",
      "{'loss': 1.9864, 'learning_rate': 3.362048046590633e-05, 'epoch': 0.66}\n",
      "{'loss': 1.9807, 'learning_rate': 3.3569926393270245e-05, 'epoch': 0.66}\n",
      "{'loss': 1.9806, 'learning_rate': 3.351937232063415e-05, 'epoch': 0.66}\n",
      "{'loss': 1.9838, 'learning_rate': 3.3468818247998063e-05, 'epoch': 0.66}\n",
      "{'loss': 1.9887, 'learning_rate': 3.341826417536197e-05, 'epoch': 0.66}\n",
      "{'loss': 1.9801, 'learning_rate': 3.336771010272588e-05, 'epoch': 0.67}\n",
      "{'loss': 1.9864, 'learning_rate': 3.331715603008979e-05, 'epoch': 0.67}\n",
      "{'loss': 1.9787, 'learning_rate': 3.3266601957453694e-05, 'epoch': 0.67}\n",
      "{'loss': 1.9781, 'learning_rate': 3.32160478848176e-05, 'epoch': 0.67}\n",
      "{'loss': 1.9859, 'learning_rate': 3.3165493812181506e-05, 'epoch': 0.67}\n",
      "{'loss': 1.9681, 'learning_rate': 3.311493973954542e-05, 'epoch': 0.68}\n",
      "{'loss': 1.978, 'learning_rate': 3.3064385666909325e-05, 'epoch': 0.68}\n",
      "{'loss': 1.9723, 'learning_rate': 3.301383159427324e-05, 'epoch': 0.68}\n",
      "{'loss': 1.9745, 'learning_rate': 3.296327752163714e-05, 'epoch': 0.68}\n",
      "{'loss': 1.9711, 'learning_rate': 3.2912723449001056e-05, 'epoch': 0.68}\n",
      "{'loss': 1.9673, 'learning_rate': 3.286216937636496e-05, 'epoch': 0.69}\n",
      "{'loss': 1.973, 'learning_rate': 3.281161530372887e-05, 'epoch': 0.69}\n",
      "{'loss': 1.9668, 'learning_rate': 3.2761061231092774e-05, 'epoch': 0.69}\n",
      "{'loss': 1.9732, 'learning_rate': 3.271050715845669e-05, 'epoch': 0.69}\n",
      "{'loss': 1.9784, 'learning_rate': 3.265995308582059e-05, 'epoch': 0.69}\n",
      "{'loss': 1.9582, 'learning_rate': 3.2609399013184505e-05, 'epoch': 0.7}\n",
      "{'loss': 1.9599, 'learning_rate': 3.255884494054841e-05, 'epoch': 0.7}\n",
      "{'loss': 1.9622, 'learning_rate': 3.2508290867912324e-05, 'epoch': 0.7}\n",
      "{'loss': 1.9623, 'learning_rate': 3.245773679527623e-05, 'epoch': 0.7}\n",
      "{'loss': 1.958, 'learning_rate': 3.2407182722640136e-05, 'epoch': 0.7}\n",
      "{'loss': 1.962, 'learning_rate': 3.235662865000404e-05, 'epoch': 0.71}\n",
      "{'loss': 1.9622, 'learning_rate': 3.2306074577367955e-05, 'epoch': 0.71}\n",
      "{'loss': 1.9573, 'learning_rate': 3.225552050473186e-05, 'epoch': 0.71}\n",
      "{'loss': 1.9601, 'learning_rate': 3.220496643209577e-05, 'epoch': 0.71}\n",
      "{'loss': 1.9526, 'learning_rate': 3.215441235945968e-05, 'epoch': 0.71}\n",
      "{'loss': 1.9517, 'learning_rate': 3.210385828682359e-05, 'epoch': 0.72}\n",
      "{'loss': 1.9574, 'learning_rate': 3.20533042141875e-05, 'epoch': 0.72}\n",
      "{'loss': 1.9497, 'learning_rate': 3.200275014155141e-05, 'epoch': 0.72}\n",
      "{'loss': 1.939, 'learning_rate': 3.195219606891531e-05, 'epoch': 0.72}\n",
      "{'loss': 1.9482, 'learning_rate': 3.1901641996279216e-05, 'epoch': 0.72}\n",
      "{'loss': 1.9555, 'learning_rate': 3.185108792364313e-05, 'epoch': 0.73}\n",
      "{'loss': 1.9476, 'learning_rate': 3.1800533851007035e-05, 'epoch': 0.73}\n",
      "{'loss': 1.9483, 'learning_rate': 3.174997977837095e-05, 'epoch': 0.73}\n",
      "{'loss': 1.9504, 'learning_rate': 3.169942570573485e-05, 'epoch': 0.73}\n",
      "{'loss': 1.9406, 'learning_rate': 3.1648871633098766e-05, 'epoch': 0.73}\n",
      "{'loss': 1.9405, 'learning_rate': 3.159831756046267e-05, 'epoch': 0.74}\n",
      "{'loss': 1.9333, 'learning_rate': 3.1547763487826585e-05, 'epoch': 0.74}\n",
      "{'loss': 1.9419, 'learning_rate': 3.149720941519049e-05, 'epoch': 0.74}\n",
      "{'loss': 1.9311, 'learning_rate': 3.1446655342554397e-05, 'epoch': 0.74}\n",
      "{'loss': 1.9364, 'learning_rate': 3.13961012699183e-05, 'epoch': 0.74}\n",
      "{'loss': 1.943, 'learning_rate': 3.1345547197282215e-05, 'epoch': 0.75}\n",
      "{'loss': 1.9407, 'learning_rate': 3.129499312464612e-05, 'epoch': 0.75}\n",
      "{'loss': 1.9408, 'learning_rate': 3.1244439052010034e-05, 'epoch': 0.75}\n",
      "{'loss': 1.9359, 'learning_rate': 3.119388497937394e-05, 'epoch': 0.75}\n",
      "{'loss': 1.9356, 'learning_rate': 3.114333090673785e-05, 'epoch': 0.75}\n",
      "{'loss': 1.9298, 'learning_rate': 3.109277683410176e-05, 'epoch': 0.76}\n",
      "{'loss': 1.925, 'learning_rate': 3.1042222761465665e-05, 'epoch': 0.76}\n",
      "{'loss': 1.9355, 'learning_rate': 3.099166868882957e-05, 'epoch': 0.76}\n",
      "{'loss': 1.9253, 'learning_rate': 3.094111461619348e-05, 'epoch': 0.76}\n",
      "{'loss': 1.9198, 'learning_rate': 3.089056054355739e-05, 'epoch': 0.76}\n",
      "{'loss': 1.9166, 'learning_rate': 3.08400064709213e-05, 'epoch': 0.77}\n",
      "{'loss': 1.9264, 'learning_rate': 3.078945239828521e-05, 'epoch': 0.77}\n",
      "{'loss': 1.9298, 'learning_rate': 3.073889832564912e-05, 'epoch': 0.77}\n",
      "{'loss': 1.9216, 'learning_rate': 3.0688344253013027e-05, 'epoch': 0.77}\n",
      "{'loss': 1.9252, 'learning_rate': 3.063779018037693e-05, 'epoch': 0.77}\n",
      "{'loss': 1.9174, 'learning_rate': 3.058723610774084e-05, 'epoch': 0.78}\n",
      "{'loss': 1.9203, 'learning_rate': 3.053668203510475e-05, 'epoch': 0.78}\n",
      "{'loss': 1.9133, 'learning_rate': 3.0486127962468657e-05, 'epoch': 0.78}\n",
      "{'loss': 1.9165, 'learning_rate': 3.0435573889832563e-05, 'epoch': 0.78}\n",
      "{'loss': 1.9265, 'learning_rate': 3.0385019817196476e-05, 'epoch': 0.78}\n",
      "{'loss': 1.9122, 'learning_rate': 3.0334465744560382e-05, 'epoch': 0.79}\n",
      "{'loss': 1.9142, 'learning_rate': 3.028391167192429e-05, 'epoch': 0.79}\n",
      "{'loss': 1.9226, 'learning_rate': 3.0233357599288197e-05, 'epoch': 0.79}\n",
      "{'loss': 1.909, 'learning_rate': 3.018280352665211e-05, 'epoch': 0.79}\n",
      "{'loss': 1.9159, 'learning_rate': 3.0132249454016016e-05, 'epoch': 0.79}\n",
      "{'loss': 1.914, 'learning_rate': 3.0081695381379925e-05, 'epoch': 0.8}\n",
      "{'loss': 1.9084, 'learning_rate': 3.003114130874383e-05, 'epoch': 0.8}\n",
      "{'loss': 1.9166, 'learning_rate': 2.9980587236107744e-05, 'epoch': 0.8}\n",
      "{'loss': 1.9089, 'learning_rate': 2.993003316347165e-05, 'epoch': 0.8}\n",
      "{'loss': 1.9093, 'learning_rate': 2.987947909083556e-05, 'epoch': 0.8}\n",
      "{'loss': 1.9001, 'learning_rate': 2.9828925018199465e-05, 'epoch': 0.81}\n",
      "{'loss': 1.9009, 'learning_rate': 2.9778370945563378e-05, 'epoch': 0.81}\n",
      "{'loss': 1.909, 'learning_rate': 2.9727816872927284e-05, 'epoch': 0.81}\n",
      "{'loss': 1.9165, 'learning_rate': 2.9677262800291193e-05, 'epoch': 0.81}\n",
      "{'loss': 1.9003, 'learning_rate': 2.96267087276551e-05, 'epoch': 0.81}\n",
      "{'loss': 1.8939, 'learning_rate': 2.9576154655019012e-05, 'epoch': 0.82}\n",
      "{'loss': 1.9015, 'learning_rate': 2.9525600582382918e-05, 'epoch': 0.82}\n",
      "{'loss': 1.8993, 'learning_rate': 2.9475046509746827e-05, 'epoch': 0.82}\n",
      "{'loss': 1.9057, 'learning_rate': 2.9424492437110733e-05, 'epoch': 0.82}\n",
      "{'loss': 1.8987, 'learning_rate': 2.9373938364474646e-05, 'epoch': 0.83}\n",
      "{'loss': 1.8985, 'learning_rate': 2.9323384291838552e-05, 'epoch': 0.83}\n",
      "{'loss': 1.9022, 'learning_rate': 2.927283021920246e-05, 'epoch': 0.83}\n",
      "{'loss': 1.9006, 'learning_rate': 2.9222276146566367e-05, 'epoch': 0.83}\n",
      "{'loss': 1.8963, 'learning_rate': 2.917172207393028e-05, 'epoch': 0.83}\n",
      "{'loss': 1.8929, 'learning_rate': 2.9121168001294186e-05, 'epoch': 0.84}\n",
      "{'loss': 1.8943, 'learning_rate': 2.9070613928658092e-05, 'epoch': 0.84}\n",
      "{'loss': 1.8958, 'learning_rate': 2.9020059856022e-05, 'epoch': 0.84}\n",
      "{'loss': 1.8967, 'learning_rate': 2.8969505783385907e-05, 'epoch': 0.84}\n",
      "{'loss': 1.8857, 'learning_rate': 2.891895171074982e-05, 'epoch': 0.84}\n",
      "{'loss': 1.8956, 'learning_rate': 2.8868397638113726e-05, 'epoch': 0.85}\n",
      "{'loss': 1.885, 'learning_rate': 2.881784356547764e-05, 'epoch': 0.85}\n",
      "{'loss': 1.8854, 'learning_rate': 2.876728949284154e-05, 'epoch': 0.85}\n",
      "{'loss': 1.8943, 'learning_rate': 2.8716735420205454e-05, 'epoch': 0.85}\n",
      "{'loss': 1.8888, 'learning_rate': 2.866618134756936e-05, 'epoch': 0.85}\n",
      "{'loss': 1.8806, 'learning_rate': 2.8615627274933272e-05, 'epoch': 0.86}\n",
      "{'loss': 1.8918, 'learning_rate': 2.856507320229718e-05, 'epoch': 0.86}\n",
      "{'loss': 1.8871, 'learning_rate': 2.8514519129661088e-05, 'epoch': 0.86}\n",
      "{'loss': 1.8924, 'learning_rate': 2.8463965057024994e-05, 'epoch': 0.86}\n",
      "{'loss': 1.8817, 'learning_rate': 2.8413410984388906e-05, 'epoch': 0.86}\n",
      "{'loss': 1.877, 'learning_rate': 2.8362856911752812e-05, 'epoch': 0.87}\n",
      "{'loss': 1.8758, 'learning_rate': 2.8312302839116722e-05, 'epoch': 0.87}\n",
      "{'loss': 1.8773, 'learning_rate': 2.8261748766480628e-05, 'epoch': 0.87}\n",
      "{'loss': 1.8731, 'learning_rate': 2.821119469384454e-05, 'epoch': 0.87}\n",
      "{'loss': 1.8744, 'learning_rate': 2.8160640621208446e-05, 'epoch': 0.87}\n",
      "{'loss': 1.8845, 'learning_rate': 2.8110086548572356e-05, 'epoch': 0.88}\n",
      "{'loss': 1.8801, 'learning_rate': 2.8059532475936262e-05, 'epoch': 0.88}\n",
      "{'loss': 1.878, 'learning_rate': 2.8008978403300174e-05, 'epoch': 0.88}\n",
      "{'loss': 1.8763, 'learning_rate': 2.795842433066408e-05, 'epoch': 0.88}\n",
      "{'loss': 1.8759, 'learning_rate': 2.790787025802799e-05, 'epoch': 0.88}\n",
      "{'loss': 1.8702, 'learning_rate': 2.7857316185391896e-05, 'epoch': 0.89}\n",
      "{'loss': 1.8699, 'learning_rate': 2.780676211275581e-05, 'epoch': 0.89}\n",
      "{'loss': 1.8778, 'learning_rate': 2.7756208040119714e-05, 'epoch': 0.89}\n",
      "{'loss': 1.8729, 'learning_rate': 2.770565396748362e-05, 'epoch': 0.89}\n",
      "{'loss': 1.8712, 'learning_rate': 2.765509989484753e-05, 'epoch': 0.89}\n",
      "{'loss': 1.8637, 'learning_rate': 2.7604545822211436e-05, 'epoch': 0.9}\n",
      "{'loss': 1.872, 'learning_rate': 2.755399174957535e-05, 'epoch': 0.9}\n",
      "{'loss': 1.8774, 'learning_rate': 2.7503437676939254e-05, 'epoch': 0.9}\n",
      "{'loss': 1.8681, 'learning_rate': 2.7452883604303164e-05, 'epoch': 0.9}\n",
      "{'loss': 1.8696, 'learning_rate': 2.740232953166707e-05, 'epoch': 0.9}\n",
      "{'loss': 1.8717, 'learning_rate': 2.7351775459030982e-05, 'epoch': 0.91}\n",
      "{'loss': 1.8607, 'learning_rate': 2.730122138639489e-05, 'epoch': 0.91}\n",
      "{'loss': 1.8685, 'learning_rate': 2.7250667313758798e-05, 'epoch': 0.91}\n",
      "{'loss': 1.8535, 'learning_rate': 2.7200113241122704e-05, 'epoch': 0.91}\n",
      "{'loss': 1.8686, 'learning_rate': 2.7149559168486616e-05, 'epoch': 0.91}\n",
      "{'loss': 1.8653, 'learning_rate': 2.7099005095850522e-05, 'epoch': 0.92}\n",
      "{'loss': 1.8685, 'learning_rate': 2.704845102321443e-05, 'epoch': 0.92}\n",
      "{'loss': 1.8617, 'learning_rate': 2.6997896950578338e-05, 'epoch': 0.92}\n",
      "{'loss': 1.8591, 'learning_rate': 2.694734287794225e-05, 'epoch': 0.92}\n",
      "{'loss': 1.8612, 'learning_rate': 2.6896788805306156e-05, 'epoch': 0.92}\n",
      "{'loss': 1.856, 'learning_rate': 2.6846234732670066e-05, 'epoch': 0.93}\n",
      "{'loss': 1.8575, 'learning_rate': 2.679568066003397e-05, 'epoch': 0.93}\n",
      "{'loss': 1.8731, 'learning_rate': 2.6745126587397884e-05, 'epoch': 0.93}\n",
      "{'loss': 1.8559, 'learning_rate': 2.669457251476179e-05, 'epoch': 0.93}\n"
     ]
    }
   ],
   "source": [
    "# DDP Train\n",
    "def train_trainer_ddp():\n",
    "    model = AutoModelForMaskedLM.from_pretrained('bert-base-uncased', config=model_config, cache_dir=model_cache_dir)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = model_output_dir,\n",
    "        logging_dir=\"runs/\"+model_output_dir,\n",
    "        do_train = True,\n",
    "        do_eval = True,\n",
    "        per_device_train_batch_size = 48,\n",
    "        per_device_eval_batch_size = 48,        \n",
    "        evaluation_strategy = \"epoch\",        \n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=5000,\n",
    "        logging_steps = 50,\n",
    "        prediction_loss_only = True,\n",
    "        learning_rate = 5e-5,\n",
    "        weight_decay = 0,\n",
    "        adam_epsilon = 1e-8,\n",
    "        max_grad_norm = 1.0,\n",
    "        num_train_epochs = 2,\n",
    "        disable_tqdm=\"false\",\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
    "                                                mlm=True, \n",
    "                                                mlm_probability=0.15,)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_datasets,\n",
    "    )   \n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "notebook_launcher(train_trainer_ddp, args=(), num_processes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb58d06",
   "metadata": {},
   "source": [
    "# Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60696eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_config = AutoConfig.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb429e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForMaskedLM.from_pretrained('bert-base-uncased', config=model_config, cache_dir=model_cache_dir)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b4911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir = model_output_dir,\n",
    "#     overwrite_output_dir = True,\n",
    "#     do_train = True,\n",
    "#     do_eval = True,\n",
    "#     per_device_train_batch_size = 32,\n",
    "#     per_device_eval_batch_size = 32,\n",
    "#     logging_steps = 50,\n",
    "#     prediction_loss_only = True,\n",
    "#     learning_rate = 5e-5,\n",
    "#     weight_decay = 0,\n",
    "#     adam_epsilon = 1e-8,\n",
    "#     max_grad_norm = 1.0,\n",
    "#     num_train_epochs = 2,\n",
    "#     save_steps = -1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
    "#                                                 mlm=True, \n",
    "#                                                 mlm_probability=0.15,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15d13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=data_collator,\n",
    "#     train_dataset=tokenized_datasets,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f364939",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
