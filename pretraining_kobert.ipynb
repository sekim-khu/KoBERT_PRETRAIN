{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e56af9ec",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73acff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Korpora import Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae7ca758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[namuwikitext] download namuwikitext_20200302.train.zip: 1.78GB [01:27, 20.4MB/s]                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip /root/Korpora/namuwikitext/namuwikitext_20200302.train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[namuwikitext] download namuwikitext_20200302.test.zip: 9.14MB [00:00, 9.51MB/s]                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip /root/Korpora/namuwikitext/namuwikitext_20200302.test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[namuwikitext] download namuwikitext_20200302.dev.zip: 8.71MB [00:02, 3.87MB/s]                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip /root/Korpora/namuwikitext/namuwikitext_20200302.dev\n"
     ]
    }
   ],
   "source": [
    "Korpora.fetch(\"namuwikitext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d385063d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : Hyunjoong Kim lovit@github\n",
      "    Repository : https://github.com/lovit/namuwikitext\n",
      "    References :\n",
      "\n",
      "    나무위키의 덤프 데이터를 바탕을 제작한 wikitext 형식의 텍스트 파일입니다.\n",
      "    학습 및 평가를 위하여 위키페이지 별로 train (99%), dev (0.5%), test (0.5%) 로 나뉘어져있습니다.\n",
      "\n",
      "\n",
      "    # License\n",
      "    CC BY-NC-SA 2.0 KR which Namuwiki dump dataset is licensed\n",
      "\n",
      "[Korpora] Corpus `namuwikitext` is already installed at /root/Korpora/namuwikitext/namuwikitext_20200302.train.zip\n",
      "[Korpora] Corpus `namuwikitext` is already installed at /root/Korpora/namuwikitext/namuwikitext_20200302.train\n",
      "[Korpora] Corpus `namuwikitext` is already installed at /root/Korpora/namuwikitext/namuwikitext_20200302.test.zip\n",
      "[Korpora] Corpus `namuwikitext` is already installed at /root/Korpora/namuwikitext/namuwikitext_20200302.test\n",
      "[Korpora] Corpus `namuwikitext` is already installed at /root/Korpora/namuwikitext/namuwikitext_20200302.dev.zip\n",
      "[Korpora] Corpus `namuwikitext` is already installed at /root/Korpora/namuwikitext/namuwikitext_20200302.dev\n",
      "NamuwikiText.train text file is large (5.3G).\n",
      "If you want to load text in your memory, please insert `yes`\n",
      "If the `INPUT` is integer, it loads only first `INPUT` sentences\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load('namuwikitext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f20707d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3910370"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b376ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePair(text=\"羽衣小町\\n《아이돌 마스터 신데렐라 걸즈》에 등장하는 코바야카와 사에, 시오미 슈코로 이루어진 2인 유닛. 통칭 '슈사에'. 둘 다 교토가 고향이고 일본 전통 컨셉 아이돌이라는 공통점이 있다. 또한 이 둘은 아이돌을 하는데 있어서 집안과의 갈등을 포함하고 있다는 공통점도 있다. 사에는 엄격한 부모님의 반대를 무릅쓰고 상경, 슈코는 화과자 집 가업을 잇기 싫어서 가출 ( ... )\\n모바마스에서 둘의 상호 아이돌 토크가 있고 '하고로모코마치'라는 유닛명을 달고 나와 여러 이벤트에서 출연하고 있다. 유닛명은 '날개옷 미녀'라는 뜻이다.\\nMAGIC HOUR SP 9화에서 슈코와 사에 단 둘이서만 나와서 직접 유닛명을 언급했다.# 신데메이션 25화에서 사에, 슈코, 아즈키, 카코가 함께 행사를 꾸리는 장면이 스치듯 지나갔다. 신데렐라 걸즈 극장 애니 5화에서 사에, 슈코가 카오루를 데리고 꽃구경을 갔다.\\n2016년 10월 15일에 열린 신데마스 4thLIVE SSA 공연에서 사에와 슈코의 성우가 青の一番星를 듀엣으로 부르는 무대가 있었다.\\n데레스테에서 사에의 개인 커뮤에 슈코가 등장하거나, 사에의 SSR에 슈코가 같이 그려지는 등 친분 관계가 지속적으로 묘사되고 있다.\\n2018년 1월 12일에 데레스테를 통해 유닛곡 美に入り彩を穿つ가 나오게 되었다.\\n특이하게도 데레스테에서 쿨 속성의 다크 일루미네이트처럼 솔로곡인 花簪 HANAKANZASHI와 青の一番星는 물론, 듀엣곡인 美に入り彩を穿つ까지 MASTER 레벨이 모두 27로 통일되어 있고, 셋 다 난이도가 27 상위권이라고 평가받고 있다. 한 손으로 짧은 롱노트+플릭을 좌우로 번갈아가면서 긋는 부분도 공통적으로 나타난다. 심지어 개별곡의 MASTER+ 조차도 똑같이 29레벨 불렙으로 나왔다.\\n2018년 11월 19일에 사에의 [하고로모코마치] SSR이, 약 2달 후인 2019년 1월 18일에 슈코의 [하고로모코마치] SSR이 등장함에 따라, 하고로모코마치도 유닛 통상 SSR을 가지게 되었다. 순서는 4번째. 뉴 제네레이션-핑크 체크 스쿨-인디비쥬얼즈-하고로모코마치 순.\", pair=' = 하고로모코마치 =')\n"
     ]
    }
   ],
   "source": [
    "print(corpus.train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6582d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8ae71fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePair(text='', pair=' = 神様になった日 =')\n"
     ]
    }
   ],
   "source": [
    "for item in corpus.train:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53acb14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50daa1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8196f05a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, load_dataset\n",
    "raw_datasets = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "# wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "# wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "\n",
    "# assert bookcorpus.features.type == wiki.features.type\n",
    "# raw_datasets = concatenate_datasets([bookcorpus, wiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f17b19d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f/cache-ddaf5e5442cf907f_*_of_00064.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'special_tokens_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import multiprocessing\n",
    "# from tqdm import tqdm\n",
    "\n",
    "'''\n",
    "## training a tokenizer from scratch\n",
    "# def batch_iterator(batch_size=10000):\n",
    "#     for i in tqdm(range(0, len(raw_datasets), batch_size)):\n",
    "#         yield raw_datasets[i:i+batch_size][\"text\"]\n",
    "        \n",
    "# old_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = old_tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=30522)\n",
    "# tokenizer.save_pretrained(\"tokenizer\")\n",
    "'''\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "\n",
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "       examples[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=tokenizer.model_max_length\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# preprocess dataset\n",
    "tokenized_datasets = raw_datasets.map(group_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "tokenized_datasets.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b41b2bf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f/cache-63d4031cf6cf3571_*_of_00064.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f/cache-dad49d059028260c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataset contains in total 1239030784 tokens\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= tokenizer.model_max_length:\n",
    "        total_length = (total_length // tokenizer.model_max_length) * tokenizer.model_max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + tokenizer.model_max_length] for i in range(0, total_length, tokenizer.model_max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(group_texts, batched=True, num_proc=num_proc)\n",
    "# shuffle dataset\n",
    "tokenized_datasets = tokenized_datasets.shuffle(seed=34)\n",
    "\n",
    "print(f\"the dataset contains in total {len(tokenized_datasets)*tokenizer.model_max_length} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b50cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "model_config = AutoConfig.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b4adb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(15000, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained('bert-base-uncased', config=model_config, cache_dir=\"./BERT_cache\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee45705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./BERT\",\n",
    "    overwrite_output_dir = True,\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    logging_steps = 50,\n",
    "    prediction_loss_only = True,\n",
    "    learning_rate = 5e-5,\n",
    "    weight_decay = 0,\n",
    "    adam_epsilon = 1e-8,\n",
    "    max_grad_norm = 1.0,\n",
    "    num_train_epochs = 1,\n",
    "    save_steps = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea461b7",
   "metadata": {},
   "source": [
    "## Train a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c63c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
    "                                                mlm=True, \n",
    "                                                mlm_probability=0.15,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e41388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f364939",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='37813' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   28/37813 00:16 < 6:47:53, 1.54 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.012200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.871000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10.278800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.616600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>8.454800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8.396900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8.270100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>8.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>8.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>7.984800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>7.797900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>7.799300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>7.699400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>7.626700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>7.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>7.447700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>7.432900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>7.238600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.265700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>7.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>7.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>6.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>7.065700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>7.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>7.002500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e000cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
