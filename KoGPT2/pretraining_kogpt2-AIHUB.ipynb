{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e56af9ec",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27f43cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "# from nltk import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoConfig, TrainingArguments, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer\n",
    "from accelerate import notebook_launcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81616053",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd55e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data\"\n",
    "original_train_datasets_path = data_path + \"/original_datasets/TS1\"\n",
    "original_valid_datasets_path = data_path + \"/original_datasets/VS1\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_time = strftime(\"%Y-%m-%d-%H:%M:%S\", gmtime())\n",
    "model_output_dir = './gpt2'+\"_\"+current_time\n",
    "model_cache_dir = \"./gpt2_cache\"\n",
    "raw_datasets_cache_name = \".raw_datasets_cache\"\n",
    "raw_datasets_cache_path = os.path.join(current_dir, raw_datasets_cache_name)\n",
    "\n",
    "train_sentence_list_file_name = \"train_sentence_list.txt\"\n",
    "valid_sentence_list_file_name = \"valid_sentence_list.txt\"\n",
    "tokenizer_name = \"tokenizer_aihub_news_gpt2\"\n",
    "tokenized_datasets_folder_name = [\"gpt2_tokenized_datasets\", \"gpt2_tokenized_datasets_1\", \"gpt2_tokenized_datasets_2\", \"gpt2_tokenized_datasets_3\"]\n",
    "grouped_tokenized_datasets_folder_name = \"gpt2_grouped_tokenized_datasets\"\n",
    "\n",
    "old_model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a29c6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f9ce7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets = load_dataset('text', data_files={\"train\": os.path.join(data_path, train_sentence_list_file_name), \"valid\": os.path.join(data_path, valid_sentence_list_file_name)}, cache_dir=raw_datasets_cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254e1e2",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a53ddb",
   "metadata": {},
   "source": [
    "### from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc0bae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_training_corpus(batch_size=10000):\n",
    "#     for dataset in [raw_datasets['train'], raw_datasets['valid']]:\n",
    "#         for start_idx in range(0, len(dataset), batch_size):\n",
    "#             yield dataset[start_idx : start_idx + batch_size][\"text\"]\n",
    "        \n",
    "# old_tokenizer = AutoTokenizer.from_pretrained(old_model_name)\n",
    "# tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus(), 51200)\n",
    "# tokenizer.save_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea97ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets_1 = raw_datasets.select(range(20000000))\n",
    "# raw_datasets_2 = raw_datasets.select(range(20000000, 40000000))\n",
    "# raw_datasets_3 = raw_datasets.select(range(40000000, len(raw_datasets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e82a304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_texts(examples):\n",
    "#     tokenized_inputs = tokenizer(\n",
    "#        examples[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=512\n",
    "#     )\n",
    "#     return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "457bb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = raw_datasets.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "# tokenized_datasets.save_to_disk(os.path.join(current_dir, tokenized_datasets_folder_name[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5f4b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_1 = raw_datasets_1.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "# tokenized_datasets_1.save_to_disk(os.path.join(current_dir, tokenized_datasets_folder_name[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "616aed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_2 = raw_datasets_2.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "# tokenized_datasets_2.save_to_disk(os.path.join(current_dir, tokenized_datasets_folder_name[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5f4ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_3 = raw_datasets_3.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "# tokenized_datasets_3.save_to_disk(os.path.join(current_dir, tokenized_datasets_folder_name[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d75675a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = concatenate_datasets([tokenized_datasets_1, tokenized_datasets_2, tokenized_datasets_1])\n",
    "# tokenized_datasets.save_to_disk(os.path.join(current_dir, tokenized_datasets_folder_name[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e29bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = load_from_disk(os.path.join(current_dir, tokenized_datasets_folder_name[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33fbc0",
   "metadata": {},
   "source": [
    "### load pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4dcf93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efd6be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_1 = load_from_disk(os.path.join(current_dir, tokenized_datasets_folder_name[1]))\n",
    "# tokenized_datasets_2 = load_from_disk(os.path.join(current_dir, tokenized_datasets_folder_name[2]))\n",
    "# tokenized_datasets_3 = load_from_disk(os.path.join(current_dir, tokenized_datasets_folder_name[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eed83b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = load_from_disk(os.path.join(current_dir, tokenized_datasets_folder_name[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ec049",
   "metadata": {},
   "source": [
    "# Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff278cc9",
   "metadata": {},
   "source": [
    "### from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1816b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# # max_seq_length.\n",
    "# model_max_length = 512\n",
    "# def group_texts(examples):\n",
    "#     # Concatenate all texts.\n",
    "#     concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "#     total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "#     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "#     # customize this part to your needs.\n",
    "#     if total_length >= model_max_length:\n",
    "#         total_length = (total_length // model_max_length) * model_max_length\n",
    "#     # Split by chunks of max_len.\n",
    "#     result = {\n",
    "#         k: [t[i : i + model_max_length] for i in range(0, total_length, model_max_length)]\n",
    "#         for k, t in concatenated_examples.items()\n",
    "#     }\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81784961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = tokenized_datasets.map(group_texts, batched=True, num_proc=num_proc)\n",
    "# # shuffle dataset\n",
    "# tokenized_datasets = tokenized_datasets.shuffle(seed=34)\n",
    "\n",
    "# print(f\"the dataset contains in total {len(tokenized_datasets)*model_max_length} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fd456e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd55ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets.save_to_disk(os.path.join(current_dir, grouped_tokenized_datasets_folder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4beda4",
   "metadata": {},
   "source": [
    "### load pre-tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45020976",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = load_from_disk(os.path.join(current_dir, grouped_tokenized_datasets_folder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be820bc",
   "metadata": {},
   "source": [
    "# DDP Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6806d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ [ \"TF_FORCE_GPU_ALLOW_GROWTH\" ] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaebc04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ddce9653764b5fab7dbc24019e68b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained(old_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b16c7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W socket.cpp:334] The server socket has failed to bind to [::]:29500 (generic error: 98 - Address already in use).\n",
      "[W socket.cpp:334] The server socket has failed to bind to 0.0.0.0:29500 (generic error: 98 - Address already in use).\n",
      "[E socket.cpp:368] The server socket has failed to listen on any local network address.\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/work/.local/lib/python3.8/site-packages/accelerate/utils/launch.py\", line 509, in __call__\n    self.launcher(*args)\n  File \"/tmp/ipykernel_267995/1258760301.py\", line 6, in train_trainer_ddp\n    training_args = TrainingArguments(\n  File \"<string>\", line 110, in __init__\n  File \"/home/work/.local/lib/python3.8/site-packages/transformers/training_args.py\", line 1259, in __post_init__\n    and (self.device.type != \"cuda\")\n  File \"/home/work/.local/lib/python3.8/site-packages/transformers/training_args.py\", line 1694, in device\n    return self._setup_devices\n  File \"/home/work/.local/lib/python3.8/site-packages/transformers/utils/generic.py\", line 54, in __get__\n    cached = self.fget(obj)\n  File \"/home/work/.local/lib/python3.8/site-packages/transformers/training_args.py\", line 1679, in _setup_devices\n    torch.distributed.init_process_group(backend=\"nccl\", timeout=self.ddp_timeout_delta)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 595, in init_process_group\n    store, rank, world_size = next(rendezvous_iterator)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 157, in _create_c10d_store\n    return TCPStore(\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (generic error: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:29500 (generic error: 98 - Address already in use).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/work/sekim_backup/KoLM_pretrain/KoGPT2/pretraining_kogpt2-AIHUB.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoGPT2/pretraining_kogpt2-AIHUB.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoGPT2/pretraining_kogpt2-AIHUB.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoGPT2/pretraining_kogpt2-AIHUB.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m         args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoGPT2/pretraining_kogpt2-AIHUB.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m         eval_dataset\u001b[39m=\u001b[39mtokenized_datasets[\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mselect(\u001b[39mrange\u001b[39m(\u001b[39m100000\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoGPT2/pretraining_kogpt2-AIHUB.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     )   \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoGPT2/pretraining_kogpt2-AIHUB.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     trainer\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoGPT2/pretraining_kogpt2-AIHUB.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m notebook_launcher(train_trainer_ddp, args\u001b[39m=\u001b[39m(), num_processes\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/launchers.py:136\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port)\u001b[0m\n\u001b[1;32m    133\u001b[0m         launcher \u001b[39m=\u001b[39m PrepareForLaunch(function, distributed_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMULTI_GPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLaunching training on \u001b[39m\u001b[39m{\u001b[39;00mnum_processes\u001b[39m}\u001b[39;00m\u001b[39m GPUs.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m         start_processes(launcher, args\u001b[39m=\u001b[39;49margs, nprocs\u001b[39m=\u001b[39;49mnum_processes, start_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfork\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    138\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[39m# No need for a distributed launch otherwise as it's either CPU, GPU or MPS.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[39mif\u001b[39;00m is_mps_available():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:198\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    197\u001b[0m \u001b[39m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    199\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:160\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-- Process \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with the following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m error_index\n\u001b[1;32m    159\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m original_trace\n\u001b[0;32m--> 160\u001b[0m \u001b[39mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[39m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/work/.local/lib/python3.8/site-packages/accelerate/utils/launch.py\", line 509, in __call__\n    self.launcher(*args)\n  File \"/tmp/ipykernel_267995/1258760301.py\", line 6, in train_trainer_ddp\n    training_args = TrainingArguments(\n  File \"<string>\", line 110, in __init__\n  File \"/home/work/.local/lib/python3.8/site-packages/transformers/training_args.py\", line 1259, in __post_init__\n    and (self.device.type != \"cuda\")\n  File \"/home/work/.local/lib/python3.8/site-packages/transformers/training_args.py\", line 1694, in device\n    return self._setup_devices\n  File \"/home/work/.local/lib/python3.8/site-packages/transformers/utils/generic.py\", line 54, in __get__\n    cached = self.fget(obj)\n  File \"/home/work/.local/lib/python3.8/site-packages/transformers/training_args.py\", line 1679, in _setup_devices\n    torch.distributed.init_process_group(backend=\"nccl\", timeout=self.ddp_timeout_delta)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 595, in init_process_group\n    store, rank, world_size = next(rendezvous_iterator)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 157, in _create_c10d_store\n    return TCPStore(\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (generic error: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:29500 (generic error: 98 - Address already in use).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# DDP Train\n",
    "def train_trainer_ddp():\n",
    "    model = AutoModelForCausalLM.from_pretrained(old_model_name, config=model_config, cache_dir=model_cache_dir)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = model_output_dir,\n",
    "        logging_dir=\"runs/\"+model_output_dir,\n",
    "        do_train = True,\n",
    "        do_eval = True,\n",
    "        no_cuda = False,\n",
    "        per_device_train_batch_size = 28,\n",
    "        per_device_eval_batch_size = 28,        \n",
    "        evaluation_strategy = \"steps\",\n",
    "        eval_steps=1000,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=5000,\n",
    "        logging_steps = 100,\n",
    "        learning_rate = 5e-5,\n",
    "        weight_decay = 0,\n",
    "        adam_epsilon = 1e-8,\n",
    "        max_grad_norm = 1.0,\n",
    "        num_train_epochs = 10,\n",
    "        disable_tqdm=\"false\",\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
    "                                                mlm=False)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['valid'].select(range(100000))\n",
    "    )   \n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "notebook_launcher(train_trainer_ddp, args=(), num_processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff69356",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
