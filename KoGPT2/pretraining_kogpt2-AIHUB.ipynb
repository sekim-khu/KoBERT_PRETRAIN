{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e56af9ec",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f43cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "# from nltk import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "\n",
    "import multiprocessing\n",
    "import parmap\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoConfig, TrainingArguments, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer\n",
    "from accelerate import notebook_launcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81616053",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data\"\n",
    "model_cache_dir = \"./gpt2_cache\"\n",
    "current_time = strftime(\"%Y-%m-%d-%H:%M:%S\", gmtime())\n",
    "model_output_dir = './BERT'+\"_\"+current_time\n",
    "original_train_datasets_path = data_path + \"/original_datasets/TS1\"\n",
    "original_valid_datasets_path = data_path + \"/original_datasets/VS1\"\n",
    "train_sentence_list_file_name = \"train_sentence_list.txt\"\n",
    "valid_sentence_list_file_name = \"valid_sentence_list.txt\"\n",
    "# raw_datasets_folder_name = \"raw_datasets\"\n",
    "raw_datasets_cache_name = \".raw_datasets_cache\"\n",
    "# raw_datasets_path = os.path.join(data_path, raw_datasets_folder_name)\n",
    "raw_datasets_cache_path = os.path.join(data_path, raw_datasets_cache_name)\n",
    "tokenizer_name = \"tokenizer_aihub_news_gpt2\"\n",
    "tokenized_datasets_folder_name = [\"gpt2_tokenized_datasets\", \"gpt2_tokenized_datasets_1\", \"gpt2_tokenized_datasets_2\", \"gpt2_tokenized_datasets_3\"]\n",
    "grouped_tokenized_datasets_folder_name = \"gpt2_grouped_tokenized_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ce7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset('text', data_files={\"train\": os.path.join(data_path, train_sentence_list_file_name), \"valid\": os.path.join(data_path, valid_sentence_list_file_name)}, cache_dir=raw_datasets_cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254e1e2",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a53ddb",
   "metadata": {},
   "source": [
    "### from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0bae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a tokenizer from scratch\n",
    "def batch_iterator(batch_size=10000):\n",
    "    for i in tqdm(range(0, len(raw_datasets), batch_size)):\n",
    "        yield raw_datasets[i:i+batch_size][\"text\"]\n",
    "        \n",
    "old_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=51200)\n",
    "tokenizer.save_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets_1 = raw_datasets.select(range(20000000))\n",
    "raw_datasets_2 = raw_datasets.select(range(20000000, 40000000))\n",
    "raw_datasets_3 = raw_datasets.select(range(40000000, len(raw_datasets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82a304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "       examples[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=tokenizer.model_max_length\n",
    "    )\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f4b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_1 = raw_datasets_1.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "tokenized_datasets_1.save_to_disk(os.path.join(data_path, tokenized_datasets_folder_name[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616aed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_2 = raw_datasets_2.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "tokenized_datasets_2.save_to_disk(os.path.join(data_path, tokenized_datasets_folder_name[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_3 = raw_datasets_3.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "tokenized_datasets_3.save_to_disk(os.path.join(data_path, tokenized_datasets_folder_name[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75675a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = concatenate_datasets([tokenized_datasets_1, tokenized_datasets_2, tokenized_datasets_1])\n",
    "tokenized_datasets.save_to_disk(os.path.join(data_path, tokenized_datasets_folder_name[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33fbc0",
   "metadata": {},
   "source": [
    "### load pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dcf93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd6be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_1 = load_from_disk(os.path.join(data_path, tokenized_datasets_folder_name[1]))\n",
    "# tokenized_datasets_2 = load_from_disk(os.path.join(data_path, tokenized_datasets_folder_name[2]))\n",
    "# tokenized_datasets_3 = load_from_disk(os.path.join(data_path, tokenized_datasets_folder_name[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed83b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = load_from_disk(os.path.join(data_path, tokenized_datasets_folder_name[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ec049",
   "metadata": {},
   "source": [
    "# Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff278cc9",
   "metadata": {},
   "source": [
    "### from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1816b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= tokenizer.model_max_length:\n",
    "        total_length = (total_length // tokenizer.model_max_length) * tokenizer.model_max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + tokenizer.model_max_length] for i in range(0, total_length, tokenizer.model_max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81784961",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.map(group_texts, batched=True, num_proc=16)\n",
    "# shuffle dataset\n",
    "tokenized_datasets = tokenized_datasets.shuffle(seed=34)\n",
    "\n",
    "print(f\"the dataset contains in total {len(tokenized_datasets)*tokenizer.model_max_length} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.save_to_disk(os.path.join(data_path, grouped_tokenized_datasets_folder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4beda4",
   "metadata": {},
   "source": [
    "### load pre-tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45020976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = load_from_disk(os.path.join(data_path, grouped_tokenized_datasets_folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be820bc",
   "metadata": {},
   "source": [
    "# DDP Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaebc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b16c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDP Train\n",
    "def train_trainer_ddp():\n",
    "    model = AutoModelForCausalLM.from_pretrained('gpt2', config=model_config, cache_dir=model_cache_dir)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = model_output_dir,\n",
    "        logging_dir=\"runs/\"+model_output_dir,\n",
    "        do_train = True,\n",
    "        do_eval = True,\n",
    "        per_device_train_batch_size = 48,\n",
    "        per_device_eval_batch_size = 48,        \n",
    "        evaluation_strategy = \"steps\",\n",
    "        eval_steps=1000,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=5000,\n",
    "        logging_steps = 100,\n",
    "        learning_rate = 5e-5,\n",
    "        weight_decay = 0,\n",
    "        adam_epsilon = 1e-8,\n",
    "        max_grad_norm = 1.0,\n",
    "        num_train_epochs = 2,\n",
    "        disable_tqdm=\"false\",\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
    "                                                mlm=False)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['valid'][:100000]\n",
    "    )   \n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "notebook_launcher(train_trainer_ddp, args=(), num_processes=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
