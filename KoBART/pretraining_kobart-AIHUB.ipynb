{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e56af9ec",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27f43cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/work/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "# from nltk import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoConfig, TrainingArguments, AutoModelForSeq2SeqLM, Trainer, DataCollatorForLanguageModeling\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "from data_collator import DataCollatorForDenoisingTasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81616053",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd55e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data\"\n",
    "original_train_datasets_path = data_path + \"/original_datasets/TS1\"\n",
    "original_valid_datasets_path = data_path + \"/original_datasets/VS1\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_time = strftime(\"%Y-%m-%d-%H:%M:%S\", gmtime())\n",
    "model_output_dir = './bart'+\"_\"+current_time\n",
    "model_cache_dir = \"./bart_cache\"\n",
    "raw_datasets_cache_name = \".raw_datasets_cache\"\n",
    "raw_datasets_cache_path = os.path.join(current_dir, raw_datasets_cache_name)\n",
    "\n",
    "train_sentence_list_file_name = \"train_sentence_list.txt\"\n",
    "valid_sentence_list_file_name = \"valid_sentence_list.txt\"\n",
    "tokenizer_name = \"tokenizer_aihub_news_bart\"\n",
    "tokenized_datasets_folder_name = [\"bart_tokenized_datasets\", \"bart_tokenized_datasets_1\", \"bart_tokenized_datasets_2\", \"bart_tokenized_datasets_3\"]\n",
    "grouped_tokenized_datasets_folder_name = \"bart_grouped_tokenized_datasets\"\n",
    "\n",
    "old_model_name = \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a29c6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f9ce7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets = load_dataset('text', data_files={\"train\": os.path.join(data_path, train_sentence_list_file_name), \"valid\": os.path.join(data_path, valid_sentence_list_file_name)}, cache_dir=raw_datasets_cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254e1e2",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a53ddb",
   "metadata": {},
   "source": [
    "### from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc0bae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_training_corpus(batch_size=10000):\n",
    "#     for dataset in [raw_datasets['train'], raw_datasets['valid']]:\n",
    "#         for start_idx in range(0, len(dataset), batch_size):\n",
    "#             yield dataset[start_idx : start_idx + batch_size][\"text\"]\n",
    "        \n",
    "# old_tokenizer = AutoTokenizer.from_pretrained(old_model_name)\n",
    "# tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus(), 30000)\n",
    "# tokenizer.save_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea97ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets_1 = raw_datasets.select(range(20000000))\n",
    "# raw_datasets_2 = raw_datasets.select(range(20000000, 40000000))\n",
    "# raw_datasets_3 = raw_datasets.select(range(40000000, len(raw_datasets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e82a304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_texts(examples):\n",
    "#     tokenized_inputs = tokenizer(\n",
    "#        examples[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=512\n",
    "#     )\n",
    "#     return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "457bb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = raw_datasets.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "# tokenized_datasets.save_to_disk(os.path.join(current_dir, tokenized_datasets_folder_name[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5f4b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_1 = raw_datasets_1.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "# tokenized_datasets_1.save_to_disk(os.path.join(current_dir, tokenized_datasets_folder_name[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "616aed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_2 = raw_datasets_2.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "# tokenized_datasets_2.save_to_disk(os.path.join(current_dir, tokenized_datasets_folder_name[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5f4ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_3 = raw_datasets_3.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "# tokenized_datasets_3.save_to_disk(os.path.join(current_dir, tokenized_datasets_folder_name[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d75675a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = concatenate_datasets([tokenized_datasets_1, tokenized_datasets_2, tokenized_datasets_1])\n",
    "# tokenized_datasets.save_to_disk(os.path.join(current_dir, tokenized_datasets_folder_name[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33fbc0",
   "metadata": {},
   "source": [
    "### load pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efd6be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_1 = load_from_disk(os.path.join(current_dir, tokenized_datasets_folder_name[1]))\n",
    "# tokenized_datasets_2 = load_from_disk(os.path.join(current_dir, tokenized_datasets_folder_name[2]))\n",
    "# tokenized_datasets_3 = load_from_disk(os.path.join(current_dir, tokenized_datasets_folder_name[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eed83b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = load_from_disk(os.path.join(current_dir, tokenized_datasets_folder_name[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ec049",
   "metadata": {},
   "source": [
    "# Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff278cc9",
   "metadata": {},
   "source": [
    "### from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1816b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# # max_seq_length.\n",
    "# model_max_length = 512\n",
    "# def group_texts(examples):\n",
    "#     # Concatenate all texts.\n",
    "#     concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "#     total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "#     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "#     # customize this part to your needs.\n",
    "#     if total_length >= model_max_length:\n",
    "#         total_length = (total_length // model_max_length) * model_max_length\n",
    "#     # Split by chunks of max_len.\n",
    "#     result = {\n",
    "#         k: [t[i : i + model_max_length] for i in range(0, total_length, model_max_length)]\n",
    "#         for k, t in concatenated_examples.items()\n",
    "#     }\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81784961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = tokenized_datasets.map(group_texts, batched=True, num_proc=num_proc)\n",
    "# # shuffle dataset\n",
    "# tokenized_datasets = tokenized_datasets.shuffle(seed=34)\n",
    "\n",
    "# print(f\"the dataset contains in total {len(tokenized_datasets)*model_max_length} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd55ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets.save_to_disk(os.path.join(current_dir, grouped_tokenized_datasets_folder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4beda4",
   "metadata": {},
   "source": [
    "### load pre-tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a76362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45020976",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = load_from_disk(os.path.join(current_dir, grouped_tokenized_datasets_folder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be820bc",
   "metadata": {},
   "source": [
    "# DDP Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaebc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(old_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b16c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DDP Train\n",
    "# def train_trainer_ddp():\n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(old_model_name, config=model_config, cache_dir=model_cache_dir)\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir = model_output_dir,\n",
    "#         logging_dir=\"runs/\"+model_output_dir,\n",
    "#         do_train = True,\n",
    "#         do_eval = True,\n",
    "#         no_cuda = False,\n",
    "#         per_device_train_batch_size = 28,\n",
    "#         per_device_eval_batch_size = 28,        \n",
    "#         evaluation_strategy = \"steps\",\n",
    "#         eval_steps=1000,\n",
    "#         save_strategy=\"steps\",\n",
    "#         save_steps=5000,\n",
    "#         logging_steps = 100,\n",
    "#         learning_rate = 5e-5,\n",
    "#         weight_decay = 0,\n",
    "#         adam_epsilon = 1e-8,\n",
    "#         max_grad_norm = 1.0,\n",
    "#         num_train_epochs = 10,\n",
    "#         disable_tqdm=\"false\",\n",
    "          report_to=\"tensorboard\"\n",
    "#     )\n",
    "\n",
    "#     data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         data_collator=data_collator,\n",
    "#         train_dataset=tokenized_datasets['train'],\n",
    "#         eval_dataset=tokenized_datasets['valid'].select(range(100000))\n",
    "#     )   \n",
    "    \n",
    "#     trainer.train()\n",
    "\n",
    "# notebook_launcher(train_trainer_ddp, args=(), num_processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a6fe220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(old_model_name, config=model_config, cache_dir=model_cache_dir)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = model_output_dir,\n",
    "    logging_dir=\"runs/\"+model_output_dir,\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    no_cuda = False,\n",
    "    per_device_train_batch_size = 28,\n",
    "    per_device_eval_batch_size = 28,        \n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=5000,\n",
    "    logging_steps = 100,\n",
    "    learning_rate = 5e-5,\n",
    "    weight_decay = 0,\n",
    "    adam_epsilon = 1e-8,\n",
    "    max_grad_norm = 1.0,\n",
    "    num_train_epochs = 10,\n",
    "    disable_tqdm=\"false\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['valid'].select(range(100000))\n",
    ")   \n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
