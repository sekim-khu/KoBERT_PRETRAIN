{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e56af9ec",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27f43cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "from nltk import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "\n",
    "import multiprocessing\n",
    "import parmap\n",
    "\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoConfig, TrainingArguments, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81616053",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd55e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data\"\n",
    "original_train_datasets_path = data_path + \"/original_datasets/TS1\"\n",
    "original_valid_datasets_path = data_path + \"/original_datasets/VS1\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_time = strftime(\"%Y-%m-%d-%H:%M:%S\", gmtime())\n",
    "model_output_dir = './BERT'+\"_\"+current_time\n",
    "model_cache_dir = \"./BERT_cache\"\n",
    "raw_datasets_cache_name = \".raw_datasets_cache\"\n",
    "raw_datasets_cache_path = os.path.join(current_dir, raw_datasets_cache_name)\n",
    "\n",
    "train_sentence_list_file_name = \"train_sentence_list.txt\"\n",
    "valid_sentence_list_file_name = \"valid_sentence_list.txt\"\n",
    "tokenizer_name = \"tokenizer_aihub_news\"\n",
    "tokenized_datasets_folder_name = [\"BERT_tokenized_datasets\", \"BERT_tokenized_datasets_1\", \"BERT_tokenized_datasets_2\", \"tokenized_datasets_3\"]\n",
    "grouped_tokenized_datasets_folder_name = \"BERT_grouped_tokenized_datasets\"\n",
    "\n",
    "old_model_name = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ecabec",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c2e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def listize_dataset(json_file_path):    \n",
    "#     sentence_list = []\n",
    "\n",
    "#     with open(json_file_path, 'r', encoding='UTF-8') as f:\n",
    "#         json_object = json.load(f)\n",
    "\n",
    "#     for line in json_object['SJML']['text']:\n",
    "#         raw_text = line['content'].replace('..', '.')\n",
    "#         raw_text_list = sent_tokenize(raw_text)\n",
    "        \n",
    "#         for item in raw_text_list:\n",
    "#             if len(item) < 5:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 sentence_list.append(item.strip())\n",
    "\n",
    "#     return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b13aae49",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parmap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/work/sekim_backup/KoLM_pretrain/KoBERT/pretraining_kobert-AIHUB.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoBERT/pretraining_kobert-AIHUB.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         json_file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(file_path, json_file_name)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoBERT/pretraining_kobert-AIHUB.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         file_path_list\u001b[39m.\u001b[39mappend(json_file_path)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoBERT/pretraining_kobert-AIHUB.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m train_sentence_list_of_list \u001b[39m=\u001b[39m parmap\u001b[39m.\u001b[39mmap(listize_dataset, file_path_list, pm_pbar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, pm_processes\u001b[39m=\u001b[39mnum_proc)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parmap' is not defined"
     ]
    }
   ],
   "source": [
    "# file_path_list = []\n",
    "\n",
    "# file_list = os.listdir(os.path.join(original_train_datasets_path))\n",
    "# for dir_name in file_list:\n",
    "#     file_path = os.path.join(original_train_datasets_path, dir_name)\n",
    "\n",
    "#     json_file_list = os.listdir(file_path)\n",
    "#     for json_file_name in json_file_list:\n",
    "#         json_file_path = os.path.join(file_path, json_file_name)\n",
    "\n",
    "#         file_path_list.append(json_file_path)\n",
    "\n",
    "# train_sentence_list_of_list = parmap.map(listize_dataset, file_path_list, pm_pbar=True, pm_processes=num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74b76f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path_list = []\n",
    "# file_list = os.listdir(os.path.join(original_valid_datasets_path))\n",
    "# for dir_name in file_list:\n",
    "#     file_path = os.path.join(original_valid_datasets_path, dir_name)\n",
    "\n",
    "#     json_file_list = os.listdir(file_path)\n",
    "#     for json_file_name in json_file_list:\n",
    "#         json_file_path = os.path.join(file_path, json_file_name)\n",
    "\n",
    "#         file_path_list.append(json_file_path)\n",
    "\n",
    "# valid_sentence_list_of_list = parmap.map(listize_dataset, file_path_list, pm_pbar=True, pm_processes=num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fb33ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sentence_list = list(chain(*train_sentence_list_of_list))\n",
    "# valid_sentence_list = list(chain(*valid_sentence_list_of_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3ab95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(data_path, train_sentence_list_file_name), 'w') as fw:\n",
    "#     for sentence in train_sentence_list:\n",
    "#         fw.write(sentence + \"\\n\")\n",
    "\n",
    "# with open(os.path.join(data_path, valid_sentence_list_file_name), 'w') as fw:\n",
    "#     for sentence in valid_sentence_list:\n",
    "#         fw.write(sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f9ce7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/work/sekim_backup/KoLM_pretrain/KoBERT/.raw_datasets_cache/text/default-1a9a91f22c227c27/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76de8128c9b247748a04cab7a2492b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a05e3d441a42e6b874afd0bef515b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f448ee6608f04081b4010d88ccbccc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371d529ea1144edfa639bfc64c20c911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/work/sekim_backup/KoLM_pretrain/KoBERT/.raw_datasets_cache/text/default-1a9a91f22c227c27/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439879019e774ffeb019c133783913b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset('text', data_files={\"train\": os.path.join(data_path, train_sentence_list_file_name), \"valid\": os.path.join(data_path, valid_sentence_list_file_name)}, cache_dir=raw_datasets_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97296a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 53742969\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 6268737\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0254e1e2",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21ef570e",
   "metadata": {},
   "source": [
    "### from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc0bae8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer_aihub_news/tokenizer_config.json',\n",
       " 'tokenizer_aihub_news/special_tokens_map.json',\n",
       " 'tokenizer_aihub_news/vocab.txt',\n",
       " 'tokenizer_aihub_news/added_tokens.json',\n",
       " 'tokenizer_aihub_news/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # training a tokenizer from scratch\n",
    "# def get_training_corpus(batch_size=10000):\n",
    "#     for dataset in [raw_datasets['train'], raw_datasets['valid']]:\n",
    "#         for start_idx in range(0, len(dataset), batch_size):\n",
    "#             yield dataset[start_idx : start_idx + batch_size][\"text\"]\n",
    "        \n",
    "# old_tokenizer = AutoTokenizer.from_pretrained(old_model_name)\n",
    "# tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus(), 15000)\n",
    "# tokenizer.save_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea97ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets_1 = raw_datasets.select(range(20000000))\n",
    "# raw_datasets_2 = raw_datasets.select(range(20000000, 40000000))\n",
    "# raw_datasets_3 = raw_datasets.select(range(40000000, len(raw_datasets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae990d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e82a304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_texts(examples):\n",
    "   #  tokenized_inputs = tokenizer(\n",
    "   #     examples[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=tokenizer.model_max_length\n",
    "   #  )\n",
    "   #  return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e259db9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b7ee9115474cebae50cc27802d050e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/53742969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a357d078a2438b8b9485381d6b10a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/6268737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized_datasets = raw_datasets.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ab0e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_1 = raw_datasets_1.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "# tokenized_datasets_1.save_to_disk(os.path.join(current_dir, tokenized_datasets_folder_name[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bff0963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_2 = raw_datasets_2.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "# tokenized_datasets_2.save_to_disk(os.path.join(current_dir, tokenized_datasets_folder_name[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efd6be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_3 = raw_datasets_3.map(preprocess_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "# tokenized_datasets_3.save_to_disk(os.path.join(current_dir, tokenized_datasets_folder_name[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eed83b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_1 = load_from_disk(os.path.join(current_dir, tokenized_datasets_folder_name[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a180cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_2 = load_from_disk(os.path.join(current_dir, tokenized_datasets_folder_name[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc8cfe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_3 = load_from_disk(os.path.join(current_dir, tokenized_datasets_folder_name[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b13958fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = concatenate_datasets([tokenized_datasets_1, tokenized_datasets_2, tokenized_datasets_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96272b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 53742969\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 6268737\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9068f00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beefb089ab41465aa91663c8fa261b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/33 shards):   0%|          | 0/53742969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fce41be32f4dc08f1acb876363fae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/5 shards):   0%|          | 0/6268737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized_datasets.save_to_disk(os.path.join(current_dir, tokenized_datasets_folder_name[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6292082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = load_from_disk(os.path.join(current_dir, tokenized_datasets_folder_name[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ec049",
   "metadata": {},
   "source": [
    "# Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1816b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# # max_seq_length.\n",
    "# def group_texts(examples):\n",
    "#     # Concatenate all texts.\n",
    "#     concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "#     total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "#     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "#     # customize this part to your needs.\n",
    "#     if total_length >= tokenizer.model_max_length:\n",
    "#         total_length = (total_length // tokenizer.model_max_length) * tokenizer.model_max_length\n",
    "#     # Split by chunks of max_len.\n",
    "#     result = {\n",
    "#         k: [t[i : i + tokenizer.model_max_length] for i in range(0, total_length, tokenizer.model_max_length)]\n",
    "#         for k, t in concatenated_examples.items()\n",
    "#     }\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81784961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1b372d173a4c23b807e9620bbe8e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/53742969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae81ec3e8519421796badf710b6b6e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/6268737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataset contains in total 1024 tokens\n"
     ]
    }
   ],
   "source": [
    "# tokenized_datasets = tokenized_datasets.map(group_texts, batched=True, num_proc=16)\n",
    "# # shuffle dataset\n",
    "# tokenized_datasets = tokenized_datasets.shuffle(seed=34)\n",
    "\n",
    "# print(f\"the dataset contains in total {len(tokenized_datasets)*tokenizer.model_max_length} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efef0d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd55ce99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f70b84ecdd43b58bcf91183648016b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/31 shards):   0%|          | 0/4215240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcd81fe75bc449d8e39fd564c6c50c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/532142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized_datasets.save_to_disk(os.path.join(current_dir, grouped_tokenized_datasets_folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45020976",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = load_from_disk(os.path.join(current_dir, grouped_tokenized_datasets_folder_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0be820bc",
   "metadata": {},
   "source": [
    "# DDP Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaebc04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7e2e8d4912429eac8bbcbe4eba780c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained(old_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b16c7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[W socket.cpp:634] The server socket on [::ffff:127.0.0.1]:29500 is not yet listening (generic error: 111 - Connection refused).\n",
      "[W socket.cpp:634] The server socket on [::ffff:127.0.0.1]:29500 is not yet listening (generic error: 111 - Connection refused).\n",
      "[W socket.cpp:634] The server socket on [::ffff:127.0.0.1]:29500 is not yet listening (generic error: 111 - Connection refused).\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[W reducer.cpp:1278] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1278] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1278] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1278] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.620137214660645, 'eval_runtime': 74.1021, 'eval_samples_per_second': 1349.489, 'eval_steps_per_second': 7.031, 'epoch': 0.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/work/sekim_backup/KoLM_pretrain/KoBERT/pretraining_kobert-AIHUB.ipynb Cell 38\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoBERT/pretraining_kobert-AIHUB.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoBERT/pretraining_kobert-AIHUB.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoBERT/pretraining_kobert-AIHUB.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m         args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoBERT/pretraining_kobert-AIHUB.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m         eval_dataset\u001b[39m=\u001b[39mtokenized_datasets[\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mselect(\u001b[39mrange\u001b[39m(\u001b[39m100000\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoBERT/pretraining_kobert-AIHUB.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     )   \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoBERT/pretraining_kobert-AIHUB.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     trainer\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnipa-ktcloud/home/work/sekim_backup/KoLM_pretrain/KoBERT/pretraining_kobert-AIHUB.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m notebook_launcher(train_trainer_ddp, args\u001b[39m=\u001b[39m(), num_processes\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/launchers.py:136\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port)\u001b[0m\n\u001b[1;32m    133\u001b[0m         launcher \u001b[39m=\u001b[39m PrepareForLaunch(function, distributed_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMULTI_GPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLaunching training on \u001b[39m\u001b[39m{\u001b[39;00mnum_processes\u001b[39m}\u001b[39;00m\u001b[39m GPUs.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m         start_processes(launcher, args\u001b[39m=\u001b[39;49margs, nprocs\u001b[39m=\u001b[39;49mnum_processes, start_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfork\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    138\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[39m# No need for a distributed launch otherwise as it's either CPU, GPU or MPS.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[39mif\u001b[39;00m is_mps_available():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:198\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    197\u001b[0m \u001b[39m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    199\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:109\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m# Wait for any process to fail or all of them to succeed.\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m ready \u001b[39m=\u001b[39m multiprocessing\u001b[39m.\u001b[39;49mconnection\u001b[39m.\u001b[39;49mwait(\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentinels\u001b[39m.\u001b[39;49mkeys(),\n\u001b[1;32m    111\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m error_index \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39mfor\u001b[39;00m sentinel \u001b[39min\u001b[39;00m ready:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# DDP Train\n",
    "def train_trainer_ddp():\n",
    "    model = AutoModelForMaskedLM.from_pretrained(old_model_name, config=model_config, cache_dir=model_cache_dir)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = model_output_dir,\n",
    "        logging_dir=\"runs/\"+model_output_dir,\n",
    "        do_train = True,\n",
    "        do_eval = True,\n",
    "        per_device_train_batch_size = 48,\n",
    "        per_device_eval_batch_size = 48,        \n",
    "        evaluation_strategy = \"steps\",\n",
    "        eval_steps=1,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=5000,\n",
    "        logging_steps = 100,\n",
    "        learning_rate = 5e-5,\n",
    "        weight_decay = 0,\n",
    "        adam_epsilon = 1e-8,\n",
    "        max_grad_norm = 1.0,\n",
    "        num_train_epochs = 2,\n",
    "        disable_tqdm=\"false\",\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
    "                                                mlm=True, \n",
    "                                                mlm_probability=0.15,)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['valid'].select(range(100000))\n",
    "    )   \n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "notebook_launcher(train_trainer_ddp, args=(), num_processes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb58d06",
   "metadata": {},
   "source": [
    "# Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60696eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_config = AutoConfig.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "feb429e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForMaskedLM.from_pretrained('bert-base-uncased', config=model_config, cache_dir=model_cache_dir)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a38b4911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir = model_output_dir,\n",
    "#     overwrite_output_dir = True,\n",
    "#     do_train = True,\n",
    "#     do_eval = True,\n",
    "#     per_device_train_batch_size = 32,\n",
    "#     per_device_eval_batch_size = 32,\n",
    "#     logging_steps = 50,\n",
    "#     prediction_loss_only = True,\n",
    "#     learning_rate = 5e-5,\n",
    "#     weight_decay = 0,\n",
    "#     adam_epsilon = 1e-8,\n",
    "#     max_grad_norm = 1.0,\n",
    "#     num_train_epochs = 2,\n",
    "#     save_steps = -1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40b4a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
    "#                                                 mlm=True, \n",
    "#                                                 mlm_probability=0.15,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f15d13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=data_collator,\n",
    "#     train_dataset=tokenized_datasets,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f364939",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6cac3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
